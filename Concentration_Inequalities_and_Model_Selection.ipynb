{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Concentration Inequalities and Model Selection.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antonFJohansson/Article_collection/blob/master/Concentration_Inequalities_and_Model_Selection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0ZsHhirh_gJ",
        "colab_type": "text"
      },
      "source": [
        "--------------------------------------------------------------\n",
        "The aim of model selection is to create a data-driven criteria to select a model from a given list. In many situations in can be useful to allow the size of the model to depend on the number of observations $n$. In these situtation the classical asymptotic theory (Akaike etc) breaks down and one needs to work with a more non-asymptotic procedure. With non-asymptotic they do not mean that a large amount of samples is not welcome, just that we can consider many models and that the size of the models can be large [Q1].\n",
        "\n",
        "Some of the main tools for this non-asymptotic theory is concentration of measure inequalities, this is since these are non-asymptotic unlike CLT and others. Some tasks where this theory can give us some benefits is\n",
        "* Density estimation: We observe some random variables with an unknown density $\\phi$ that we want to estimate.\n",
        "* Regression: We want to unfer a function $s$ such that $Y_i = s(X_i) + \\epsilon_i$.\n",
        "* Classification, Gaussian White Noise etc\n",
        "We can see these situations as observing a random variable $\\xi$ with some unknown distribution that depends on a quantity $s$ which we want to estimate. $s$ can then belong to infinite dimensional spaces or other spaces, the goal is to infer a good estimate of it.\n",
        "\n",
        "One way to estimate $s$ is to use what is known as **Minimum contrast estimation**\n",
        "\n",
        "What we do is that we consider some empirical criterion $\\gamma_n$ (which is based on the observations $\\xi^{(n)}$), such that on the set $S$ we have that the function $t \\rightarrow\\mathbb{E}[\\gamma_t(t)]$ is minimized at $s$ (where $s$ is the true underlying connection between the observations and the density). Such a criteria is called an empirical contrast for the estimation of $s$. So what we do is that we choose some subset $\\hat{S}$ of $S$ [Q2] and choose the $\\hat{s}$ that minimizes $\\gamma_n$ over $\\hat{S}$. This is precisely what we do with MLE estimation and such, our criteria $\\gamma_n$ is then the loglikelihood.\n",
        "\n",
        "What we do now is to show that some given empirical criterions $\\gamma$ are indeed empirical criterions, i.e, they minimize the desired expression at $s$. We do this by showing that the function\n",
        "$$l(s,t) = \\mathbb{E}(\\gamma_n(t)) - \\mathbb{E}(\\gamma_n(s))$$\n",
        "is non-negative for all $t \\in \\hat{S}$. For observations of random vectors $\\xi = (\\xi_1,....,\\xi_n)$ we define the empirical criterion to be \n",
        "$$\\gamma_n = \\frac{1}{n}\\sum_{i=1}^n\\gamma(t,\\xi_i)$$\n",
        "Thus we only need to specify what $\\gamma$ is for each situtation.\n",
        "* Density estimation: The choice $\\gamma(t,x) = -\\ln (t(x))$ leads to the maximum likelihood criterion and the loss function $l$ is given by\n",
        "$$l(s,t) = KL(s,t)$$\n",
        "If $s \\in L^2$ then it is also possible to define a least squares criterion by setting $\\gamma(t,x) = ||t||^2-2t(x)$, this will yield the loss function $||s-t||^2$ for all $t \\in L^2$ [Q3]\n",
        "* Regression: Here we choose the criteria\n",
        "$$\\gamma(t, (x,y)) = (y-t(x))^2$$\n",
        "which will yield the loss function $l(s,t) = ||s-t|^2|$ This follows since we take the expectation over the pair $(X,Y)$. Thus we get\n",
        "$$\\mathbb{E}[(Y-t(X))^2] - \\mathbb{E}[(Y-s(X))^2]$$\n",
        "And then just use the definition that $Y=s(X) + \\epsilon$ and that the noise is independent and zero mean to get the desired formula. [Both of these examples are interesting since we can see what kind of criterion to optimize in order to get certain structure on our solution.]\n",
        "* Classification: We take the same criterion as in the regression case but restrict our functions to be in the space of $\\{0,1\\}$-valued measurable functions. We can thus write\n",
        "$$(Y_i - t(X_i))^2 = \\mathbb{I}_{Y_i \\neq t(X_i)}$$\n",
        "The corresponding minimization can also be called **empirical risk minimization**. If the true underlying function is given by\n",
        "$$s(x) = \\mathbb{I}_{\\eta(x)\\geq 1/2}$$\n",
        "where $\\eta$ denotes the regression function $\\eta(x) = \\mathbb{E}(Y|X=x)$, the corresponding loss is given by [Q4]\n",
        "$$l(s,t) = \\mathbb{P}(Y \\neq t(X)) - P(Y \\neq s(X)) = \\mathbb{E}(|2\\eta(X)-1||s(X)-t(X))$$\n",
        "\n",
        "### Questions/Answers\n",
        "* [Q1]: With size of model I assume they mean the amount of parameters here? And then can we not consider a large amount of models with Akaike for example? Or is there an issue with something like multiple hypothesis testing then?\n",
        "* [Q2]: They say that they call $\\hat{S}$ a model?\n",
        "* [A2]: I think the reason they call this might be that it could be a Gaussian model and $\\hat{S}$ is all the different parameters that we allow for etc.\n",
        "* [Q3]: This is not clear to me that we get this loss function if we choose this criteria actually.\n",
        "* [A3]: Since what we do is density estimation we know that the true $s$ is what generates the data and thus when we take the expectation we can use this fact.\n",
        "* [Q4]: For me it is not clear how we get the second step here, to the expectation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkZ47OldRc3c",
        "colab_type": "text"
      },
      "source": [
        "----------------------------------------------------------------------\n",
        "The main problem that arises from the examples above is thus the choice of model space $S$ on which the minimum contrast estimator is defined, i.e, it can be difficult to guess what kind of parametric model that accurately captures the behaviour of the data. If one chooses to small of a set $S$ then one can get the issue that the solution is too far from the real $s$, but if one then instead chooses all of $S$ then one can get suboptimal estimators or inconsistent estimators [Q1]. When working with these models we always have the issue of the bias-variance tradeoff. They then consider the issue of selecting a sequence of models $\\{Sm\\}_{m \\in M}$, we represent each model by the minimum contrast estimator $\\hat{s}_m$ related to $\\gamma_n$. The purpose is to select the best model among the collection $\\{\\hat{s}_m\\}_{m\\in M}$.\n",
        "\n",
        "Ideally we would like to consider the model space $m(s)$ that minimizes the risk $\\mathbb{E}(l(s,\\hat{s}_m))$[Q5] (this is exactly what we do before as well when we find a $t$ but we are restricted to the set of models $m \\in M$ now.) The minimum contrast estimator $\\hat{s}_{m(s)}$ on the corresponding model space (they call this just a model) $S_{m(s)}$ is called an oracle. [Q2] The oracle is unfortunately not an estimator of $s$ though since it depends on the unknown $s$. However we can use the risk of the oracle as a benchmark for any model selection procedure that we create for the selection between the estimators $\\{\\hat{s}_m\\}_{m\\in M}$. Here they say that this notion of an oracle is different from that of the true model and that if $s$ was in some $S_{m_0}$ then that would not imply that $\\hat{s}_{m_0}$ would be an oracle. [Q3]. They now argue that the idea is to consider a data driven criteria which will select an estimator which tends to mimic an oracle, thus one would like the risk of the selected estimator be as close as possible to that of the oracle [Q4]\n",
        "\n",
        "###Questions/Answers\n",
        "* [Q1]: I take it as we overfit in this regime\n",
        "* [A1]: I believe so since what we do is minimize the empirical criterion and if we choose a function from all continuous functions then we can easily get zero error on the training set, thus we overfit and get a suboptimal estimator.\n",
        "* [Q2]: So an oracle is the best model that exists in the sequence of spaces that we consider?\n",
        "* [Q3]: Is this because we might not find the true model when we create our estimator, like if the true model is a third degree polynomial but the optimal model that we find from the data is a first degree polynomial then the first degree polynomial would be our oracle?\n",
        "* [Q4]: Why do we want this? Why do we not just require that the risk of the selected estimator is as close as possible to the true one? I also wondered why we didn't always get the oracle since that is the optimal model but I think the oracle is the one that minimizes the expectation which we cannot calculate.\n",
        "* [Q5]: But what do we take this expectation to? $s$ is the true function and should thus be fixed? I take the expectation as being with respect to the possible models $m$. But we want to find the $m$ that minimizes the expression right? So the expectation is over the possible true functions $s$ for different tasks?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNIA0s08bT81",
        "colab_type": "text"
      },
      "source": [
        "---------------------------------------------------------\n",
        "Model selection via penalization\n",
        "\n",
        "In this procedure we select a model by finding the model $\\hat{m}$ that minimizes the expression\n",
        "$$\\gamma_n(\\hat{s}_m) + pen(m)$$\n",
        "where $pen(m)$ is a penalization score (we want to penalize higher order polynomials more for example etc).\n",
        "Examples of this would be the penalization scores\n",
        "* Akaike: $D_m / n$\n",
        "* Mallows $C_p$ : $2D_m\\sigma^2 / n$\n",
        "Akaike made the assumption that the dimensions and number of models considered are bounded with respect to $n$ and that $n$ tends to infinity. (I assume thus that we are only guaranteed to select the correct model given that we consider $k$ models and that we then observe more and more data? It cannot depend on $n$ at least I guess so we get more and more models the more data we observe).\n",
        "\n",
        "An example where these assumptions are not satisfied can be given through:\n",
        "\n",
        "\n",
        "This situation is known as change points detection on the mean. We observe a noisy signal $\\xi_j$ at each time $j/n$ on [0,1]. Thus the model is\n",
        "$$\\xi_j = s(j/n) + \\epsilon_j~~ 1\\leq j \\leq n$$\n",
        "where the errors are i.i.d zero mean as usual. Detecting change points on the mean amounts to selecting the best piecewise constant estimator of the true signal $s$ with some arbitrary partition $m$ with endpoints on the grid given by $j/n$ [Q1]. Let $S_m$ be the linear space [Q2] of piecewise constant functions on partition $m$, this means that we want to select a model among the family $(S_m)_{m \\in M}$ where $M$ is the collection of all possible partitions of intervals with endpoints on the grid. (so we want to find the best piecewise constant function, and we select both which partition to use but also find the best possible model in that optimal partition.) Then the number of models with dimension $D$, i.e the number of partitions with $D$ pieces is equal to ${n-1}\\choose{D-1}$ which grows in $n$. (I think here that they mean a partition where all pieces NEED to have endpoints at $j/n$, thus no interval in the partition can be completely between two $j/n$ or end midway through. Either it is between two such points or it skips to the next adjacent such point or beyond). That is why the amount of models is then given by ${n-1}\\choose{D-1}$ since we have $D-1$ left endpoints to choose out of $n-1$ possible ones. The important lesson is that our model space grows with the amount of observations $n$.\n",
        "\n",
        "This approach to model selection through penalization differens from the usual parametric approach in the sense that\n",
        "* The amount of models and the dimension of models can depend on $n$.\n",
        "* They write that we can choose a model based on its approximation property so a neural network for example [Q3]\n",
        "They say that a common way to penalize these models is to choose a penalization given as\n",
        "$$(C_1 + C_2L_m)\\frac{D_m}{n}$$\n",
        "where $L_m$ is a weight term that penalizes the model class (like $L_2$ I assume) and often fulfills the requirement that\n",
        "$$\\sum_{m \\in M}e^{-L_mD_m} \\leq 1$$\n",
        "$C_1$ and $C_2$ do not depend on $n$.\n",
        "\n",
        "What we will see in the future is that concentration inequalities are deeply involved in the construction of these penalized criteries but also in the study of the resulting penalized estimator $\\hat{s}_{\\hat{m}}$.\n",
        "\n",
        "\n",
        "### Questions/Answers\n",
        "* [Q1]: Why do we only work with piecewise constant functions here? Or is that part of the model assumption? I guess it is just some assumption we make to find an easy counterexample to Akaikes assumptions.\n",
        "* [Q2]: What do they mean with linear space here? They mean a vector space I think.\n",
        "* [Q3]: Do they mean that this is not possible in usual parametric model selection since the best model will then be the one that fits the data completely? But with a penalization term we will deal with this issue?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqTHEFqzjZ8d",
        "colab_type": "text"
      },
      "source": [
        "----------------------------------------------------------------------\n",
        "The role of concentration inequalities\n",
        "\n",
        "Our procedure is to take a loss function $l(s,t)$ and then try and mimic the oracle, i.e, minimize $\\mathbb{E}[l(s,\\hat{s}_m)]$ over $m \\in M$. Let us introduce the centered empirical process\n",
        "$$\\bar{\\gamma_n}(t) = \\gamma_n(t) - \\mathbb{E}[\\gamma_n(t)]$$\n",
        "(I assume the expectation is taken w.r.t the data generating process?). By definition a penalized estimaot $\\hat{s}_{\\hat{m}}$ satisfies for every $m \\in M$ and any point $s_m \\in S_m$\n",
        "$$\\gamma_n(\\hat{s}_{\\hat{m}}) + pen(\\hat{m}) \\leq \\gamma_n(\\hat{s}_{m}) + pen(m) \\leq  \\gamma_n(s_{m}) + pen(m)$$ \n",
        "From my understanding these are what the terms are\n",
        "* $\\hat{s}_m$ the optimal $s$ minimizing the empirical loss for the model space $m$, without penalization.\n",
        "* $\\hat{s}_{\\hat{m}}$ the optimal $s$ in the optimal $m$, so the absolutely best function that we could find in the considered models.\n",
        "* $s_m$ Any point in $S_m$.\n",
        "Thus the inequalities follow by definitions, the first one since we choose the best $s$ in the optimal model space $m$, thus it must have a lower score than if we chose $s$ from a suboptimal space, and this second $\\hat{s}_m$ is the optimal w.r.t the empirical risk in the model space $m$ and thus it must be better than any other point that that space. Equivalently we can subsitute $\\bar{\\gamma_n(t)} + \\mathbb{E}[\\gamma_n(t)]$ to $\\gamma_n(t)$ and thus obtain\n",
        "$$\\bar{\\gamma_n(\\hat{s}_{\\hat{m}})} + pen(\\hat{m}) +  \\mathbb{E}[\\gamma_n(\\hat{s}_{\\hat{m}})] \\leq \\bar{\\gamma_n(s_m)} + pen(m) +  \\mathbb{E}[\\gamma_n(s_m)]$$\n",
        "Subtracting $\\mathbb{E}[\\gamma_n(s)]$ on both sides lead to the following important bound\n",
        "$$l(s,\\hat{s}_{\\hat{m}}) \\leq l(s,s_m) + pen(m) + \\bar{\\gamma}_n(s_m) - \\bar{\\gamma}_n(\\hat{s}_{\\hat{m}}) - pen(\\hat{m})$$\n",
        "Hence they argue that the penalty should be\n",
        "* Heavy enough to annihilate any fluctations of $\\bar{\\gamma}_n(s_m) - \\bar{\\gamma}_n(\\hat{s}_{\\hat{m}}) $\n",
        "* But not too large since ideally we would like that $l(s,s_m) + pen(m) \\leq \\mathbb{E}[l(s,\\hat{s}_m)]$ [Q1]\n",
        "Therefore in order to be able to cancel the fluctations with the penalty we need a good idea of how these fluctations $\\bar{\\gamma}_n(s_m) - \\bar{\\gamma}_n(\\hat{s}_{\\hat{m}}) $ behave.\n",
        "\n",
        "And the key to get an idea of these fluctations is to use these concentration inequalities to analyze the uniform deviation of $\\bar{\\gamma}_n(u) - \\bar{\\gamma}_n(t) $ when $u$ and $t$ are close and belongs to a given model [Q2]. Thus the key they argue is to get good control of the supremum of some conveniently weighted empirical process [Q3]\n",
        "$$\\frac{\\bar{\\gamma}_n(u) - \\bar{\\gamma}_n(t)}{a(u,t)}~~t \\in S_{m'}.$$\n",
        "We will see bounds for this that can help us.\n",
        "\n",
        "\n",
        "### Questions/Answers:\n",
        "* [Q1]: I do not follow here, what is the goal that we want? Do we want the loss to the optimal penalized solution to be less than all other possible solutions over all $m$? Maybe I could see it that way, so we just want to find a solution that is optimal in some way, i.e, closest to the true generating $s$. But we take a detour to acheive this with the penalization but we want some guarantee on our penalized solution?\n",
        "* [Q2]: I do not actually understand here. First off I think uniform deviation is the difference between the expected loss and the loss on an empirical quantitity. Why do we need $u$ and $t$ to be close together? And should they belong to the same model?\n",
        "* [Q3]: Why do we weigh this process and why do we need to control the supremum? I guess it has something to do with the relation to uniform deviation but it is not clear to me right now.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_uDXJMdS6RT",
        "colab_type": "text"
      },
      "source": [
        "Concentration inequalities\n",
        "\n",
        "The task is as follows:\n",
        "* Given independent random variables $X_1,...,X_n$ taking their values in $\\mathcal{X}^n$ (the entire vector lies in this space I assume and not the individual components?) and some functional $\\chi: \\mathcal{X}^n \\rightarrow \\mathbb{R}$, we want to study the concentration property of $Z = \\chi(X_1,...,X_n)$ (so I guess $\\chi$ could be the empirical loss or something similar and we are interested to find out how much it deviates from its expectation) around its expectation. We will see some sub-Gaussian inequalities, thus we would like to prove inequalities of the type\n",
        "$$\\mathbb{P}(Z-\\mathbb{E}[Z]\\geq x) \\leq \\exp \\big(-\\frac{x^2}{2v}\\big), 0 \\leq x \\leq x_0$$\n",
        "(So we want the probability that it deviates some quantity from its expectation to be exponentially decaying, it is a bit confusing to me why there is no absolute value in the probability though. I think the quantity $Z$ is always positive, but I guess it can still vary around the mean.) Ideally they say that one would like $v = Var(Z)$ and $x_0 = \\infty$ [Q1]. More reasonable we will content ourselves with $v$ that are good upper bounds on the Variance and also $x_0$ that is an explicit function of $n$ and $v$.\n",
        "\n",
        "In special cases we can get exactly what we required above, namely if $\\mathcal{X}^n = \\mathbb{R}^n$ is equipped with the canonical euclidiean norm, $X_1,...,X_n$ are i.i.d standard normal and $\\chi$ is assumed to be Lipshitz so [Q2]\n",
        "$$|\\chi(y) - \\chi(y')| \\leq L||y-y'||~~ \\forall y,y' \\in \\mathbb{R}^n$$\n",
        "Then we will have that $Var(Z) \\leq L^2$ and on the other hand, the Cirelson-Ibragimov-Sudakov inequality gives that\n",
        "$$\\mathbb{P}(Z-\\mathbb{E}[Z]\\geq x) \\leq \\exp \\bigg(-\\frac{x^2}{2L^2}\\bigg)~~\\forall x \\geq 0.$$\n",
        "They say that a remarkable thing about this inequality is that the dependence on $n$ is entirely within the expectation (I guess $L$ does not depend on the dimension then maybe?). It is not clear how to extend this to more general spaces, one can extend it to Hamming distances as well. \n",
        "\n",
        "Now we will return to the functional that naturally arises from the study of penalized model selection criterias. We want to analyze processes of the form (this is a functional of independent random variables).\n",
        "$$Z = \\sup_{t \\in T}\\sum_{i=1}^nf_t(X_i)$$\n",
        "Assuming that $\\sup_{t \\in T} ||f_t||_{\\infty} \\leq 1$\n",
        "gives us the Lipschitz property w.r.t the Hamming distance which gives the inequality\n",
        "$$\\mathbb{P}(Z-\\mathbb{E}[Z]\\geq x) \\leq \\exp \\big(-\\frac{x^2}{2n}\\big)~~\\forall x \\geq 0.$$\n",
        "However it may happen that the variables $f_t(X_i)$ have smal variance and thus the bound may be too crude, this is I assume since Lipschitz is a global property and cannot capture the desired local properties we need. This local behaviour lies at the heart of our analysis and thus we need to do something else. One can instead improve this bound in a different way with a different inequality.\n",
        "\n",
        "# I quit here since the book seems a bit too theoretical for me and maybe not exactly what I am after. Around page 10-12\n",
        "\n",
        "### Questions/Answers\n",
        "* [Q1]: I understand why we would like $x_0 = \\infty$ but why do we want the $v$ to be the variance? Maybe because then it is easier to analyze? They write later that they will be happy with an upper bound on the variance and I can maybe understand that.\n",
        "* [Q2]: Why does it matter which norm that the space has? Or is it because of the Lipschitz property?"
      ]
    }
  ]
}