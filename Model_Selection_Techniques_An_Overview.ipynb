{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model Selection Techniques - An Overview.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antonFJohansson/Article_collection/blob/master/Model_Selection_Techniques_An_Overview.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kXk1tGNlFQo",
        "colab_type": "text"
      },
      "source": [
        "----------------------------------------------------------------\n",
        "###What I learned from the article\n",
        "\n",
        "* One way to evaluate the predictive performance of a method (analytically) is to calculate the expected out-of-sample prediction loss\n",
        "$$E_*(s(\\hat{p}_m,Z)) = \\int s(\\hat{p}_m(z),z)p_*(z)dz$$\n",
        "(We often try and approximate this quantity with proxies such as test sets etc)\n",
        "* A model selection procedure is consistent if the true model is selected with probability going to 1 as $n \\rightarrow \\infty$. What this means in practical sense (since often reality is too complicated for the true model to be among the ones considered) is that we obtain a **stable** model which important variables are identified and if a similar study was performed again we would obtain the same model again. This criteria is often used when one is interested in inference and explaining reality. I believe **strong consistency** is that this convergence holds almost surely.\n",
        "* A model selection procedure is asymptotically efficient if\n",
        "$$\\frac{\\min_{m \\in \\mathbb{M}}L_m}{L_{\\hat{m}}} \\rightarrow_p 1$$\n",
        "What this means is that asymptotically we will select a model that has the same predictive performance as the true model. (Just one confusing note is that our set $\\mathbb{M}$ needs to contain the true model? They assumed this was the case though in parametric settings.) One can thus see that consistency implies asymptotic efficiency since we will select the true model in the consistent case, given that we are in the parametric setting.\n",
        "* In a typical settings where **the loss is logarithmic**, AIC works by selecting the model that minimizes\n",
        "$$AIC_m = -2\\hat{l}_{n,m} + 2d_m$$\n",
        "An extension of AIC that allows for miss-specified models (so the true model is not considered?) is the Takeuchi Information Criterion, but this is too difficult too work with in practical situations.\n",
        "* BIC is asymptotically equivalent to selecting the model with the highest marginal likelihood.\n",
        "* One can show that any penalty smaller than $2d_m\\log \\log n$ will not be consistent.\n",
        "* Bridge criterion tries to bridge AIC and BIC, it has consistency property for small models and if it turns out that the model is infinite dimensional the structure is such that it still achieves asymptotic efficiency.\n",
        "* Other ways to select models are highest posterior probability, Bayes Factors, marginal likelihood. If all model priors are the same, the model with highest marginal likelihood also is the model with the largest posterior probability.\n",
        "* Minimum Message Length and Minimum Description Length are two information theoretic model selection techniques. MML is fully Bayesian.\n",
        "* There exists fast version of LOO such as generalized cross-validation.\n",
        "* Predictive Least Squares (PLS) is a method for model selection for time-series where we iteratively fit the model on all data before $t$ and see how it predicts outcome $t$, and we do that for all $t$ such that the parameters can be uniquely defined.\n",
        "* There exists a general information criteria that has AIC and BIC as special cases.\n",
        "* AIC works well in non-parametric settings where it is asymptotically efficient, BIC is consistent in a parametric setting. BIC is non asymptotically efficient in non-parametric setting and no model is consistent in a non-parametric setting.\n",
        "* There is also some interest into evalutation if a framework is parametric or non-parametric, one can develop quantities which indicate asymptotically if a framework is parametric or non-parametric.\n",
        "* In many of the proofs of these information criterias they assume that the model dimension is given as $o(\\sqrt{n})$, thus one need different machinery for high-dimensional settings.\n",
        "* k-fold CV is often unstable in the sense that dividing the data into different ways will yield very different results.\n",
        "* For asymptotic efficiency, the ratio between the validation set and training set should either converge to zero or diverge to infinity depending on if we are working in parametric or non-parametric setting\n",
        "* They do a case study for neural networks and find that the smaller the validation set is the better the network performs on unseen data. So in this case the ratio should go to 0, they say that neural networks are almost non-parametric so this is not surprising.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XKvAA3yMtG1",
        "colab_type": "text"
      },
      "source": [
        "------------------------------------\n",
        "The article deals with methods to select a model from a class of models, examples are\n",
        "* number of variables in linear regression\n",
        "* order of autoregressive process\n",
        "* number of neurons in neural network etc\n",
        "\n",
        "They will focus on two goals, prediction and inference. The notation they employ is such that $M_m = \\{p_{\\theta_m}:\\theta_m \\in H_m\\}$ denotes a model which is a set of probability densities to describe the data $z_1,...,z_n$. $H_m$ is a parameter space associated with $M_m$ (so $M_m$ could be a certain form of Gaussian and $H_m$ is the real numbers for the mean I guess). A model class is a set of models so that $\\{M_m\\}_{m \\in \\mathbb{M}}$. The number of models we consider (cardinality of $\\mathbb{M}$) can depend on $n$ or be fixed. $d_m$ will denote the dimension of the parameters in $M_m$. The log-likelihood is given as $l_{n,m}(\\theta_m) = \\log p_{\\theta_m}(z_1,...,z_n)$, and the MLE for a given model is given by $\\hat{\\theta}_m$, the likelihood at the MLE will be denoted $\\hat{l}_{n,m}$. $p_*$ and $E_*$ are the true density and the expectation under the true density.\n",
        "\n",
        "In the parametric framework they say that there exists some $m \\in \\mathbb{M}$ such that there is some $\\theta_* \\in H_m$ such that $p_*$ is exactly $p_{\\theta_*}$ (so we assume the true model is included among those considered?). This assumption does not hold in the non-parametric framework.\n",
        "\n",
        "If the datageneratin procedure is in the model class $\\{M_m\\}_{m \\in \\mathbb{M}}$ we call it well-specified (and miss-specified otherwise).\n",
        "\n",
        "Some models do not consider probability distributions such as nearest neighbours but these models are not included in this framework?\n",
        "\n",
        "We fit the model by minimzing a loss-function that depends on the data in some way such as MLE. Denote the MLE model by $\\hat{p}_m = p_{\\bar{\\theta_m}}$ where $\\bar{\\theta}_m$ is the MLE solution for model $M_m$. The predictive performance of this solution can be assesed by the out-of-sample prediction loss given as ($s$ is the loss or the score function)\n",
        "$$E_*(s(\\hat{p}_m,Z)) = \\int s(\\hat{p}_m(z),z)p_*(z)dz$$\n",
        "In light of this definition, we could define the **best model** to be the model that minimizes this out-of-sample prediction score, call this model $\\hat{m}_0$ (note that the best model here is defined in terms of the models that we consider, the available data and the loss-function) (if we work with neural networks for example I assume that we just see $\\hat{p}_m$ as a model that we obtained during SGD and no requirements on that it is in a minimum or something similar?).\n",
        "\n",
        "Consistency in model selection means that we select the true model with probability one as the sample size increases (given that we assume that the true model is in the models that we consider).\n",
        "\n",
        "[They write that additionally, the MLE of $p_{\\theta_m}$ for $\\theta_m \\in H_m$ is known to attain the Cramer-Rao lower bound asymptotically but is this just for the true model or is this for all models? It seems a bit strong if it was for all models since we could consider some extremely bad models. What this says is that the minimum variance of the MLE estimate will be lower bounded and attains it asymptotically. I am just a bit confused since if we converge to the true model why do we not get zero variance?]\n",
        "\n",
        "For non-parametric models one usually has that the more data one has the better the model gets and also the larger the dimension of the model becomes. Thus consistency becomes unachievable in this regard since the model will continue to grow (but cannot there be some sort of model limit when $n \\rightarrow \\infty$?).\n",
        "\n",
        "If one is in a setting where the number of variables is large compared to the amount of data (in a regression setting for example), then these results above do not necessarily hold true and the parametric framework is closer to that of being non-parametric.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycGY3NzNVJOB",
        "colab_type": "text"
      },
      "source": [
        "------------------------------------------------------------------------------------------------\n",
        "Goals of data analysis and model selection\n",
        "\n",
        "There are two main goals of model selection, inference and prediction. In inference we want to find a model that can accurately explain why the world behaves as it does and can help practioners see what effects are important. In prediction we do not care about obtaining the true model but just aim to get a model that can predict well.\n",
        "\n",
        "For the first goal of inference they write that the concept of **selection consistency** is important which says that the best model is selected with probability 1 as $n\\rightarrow \\infty$. So basically that one would be able to identify the true underlying causes for the observed data.\n",
        "\n",
        "In many other applications, the predictive performance is what is the goal and even though the best model cannot be selected with high probability, many other models can give a similar performance. Thus the concept of **asymptotic efficiency** arises which says that the loss of the selected method or model should be asymptotically equivalent to the smallest among all candidates (with the smallest among all candidates I guess they somehow include the true model there? Or I mean otherwise what if I just consider one model and try to select that one, then that will by asymptotic efficient? I guess one looks for model selection criterias that are not trivially asymptotic efficient then maybe)\n",
        "\n",
        "A model is asymptotically efficient if\n",
        "$$\\frac{\\min_{m\\in \\mathbb{M}} L_m}{L_{\\hat{m}}} \\rightarrow_p 1$$\n",
        "where $\\hat{m}$ is the selected model $L_m = E_*(s(\\hat{p}_m,Z)) - E_*(s(p_*,Z))$. The subtraction of the second quantity will allow for better comparisons between different model selection criterias (why?, because different loss functions might give completely different values and only by comparing them to how the true model behaves can one get a good idea maybe). Other possible criterias are minimax.rate optimality and structural risk minimzation where one tries to bound the out-of-sample prediction loss by the in-sample loss + a term e.g a term depending on the VC-dimension. The major difference between the current setting and that in statistical learning (with VC-dimension and such) is the stronger requirement that the selected model should exhibit prediction loss comparable to that of the best candidates, so in statistical learning the in-sample loss + the positive term should approach the true out-of-sample loss as $n \\rightarrow \\infty$ (but they might not have such a requirement to begin with, but maybe their goal is to understand what a **single model can do, and not asymptotics results**).\n",
        "\n",
        "They are mainly concerned with finding $M_m$ since the fitting part is straight forward according to them( I guess the idea is straightforward to just find the minimum of the loss and such but it is harder to discern between all models.) If the models are nested then the model selection problem can be known as an **order selection problem**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYs9leGFbsni",
        "colab_type": "text"
      },
      "source": [
        "-----------------------------------\n",
        "We have an example here of an autoregressive model that we fit to a time-series data. Each model is of the form\n",
        "$$z_t = \\sum_{i=1}^k\\phi_{k,i}z_{t-i} + \\epsilon_t$$\n",
        "We estimate the parameters by least squares. We want to investigate this model and see if the asymptotic efficiency holds and also how large the out-of-sample error is. The out of sample error was given as\n",
        "$$E_*(s(\\hat{p}_m,Z)) = \\int s(\\hat{p}_m(z),z)p_*(z)dz$$\n",
        "Using the quadratic loss the loss term is given as\n",
        "$$s(p,z_t) = (z_t - \\mathbb{E}(z_t|z_1,...,z_{t-1}))^2$$\n",
        "where the expectation is taken w.r.t the joint distribution of $(z_1,...,z_t)$. We thus obtain $E_*(s(\\hat{p}_m,Z)) = E_*[(Z_t - \\sum_{i=1}^k\\hat{\\phi_{k,i}}Z_{t-i})^2]$. This expectation is taken w.r.t the true data-generating distribution $p_*$ [Q1]. For the expectation for the loss with the true data-generating function $p_*$ it feels like the second term should equal $Z_t$, which it does and the only remaining error comes from the intrinsic noise so we get that it evaluates to $\\sigma^2$. We can also look at the asymptotic efficiency.\n",
        "\n",
        "If we find the true model or not depends a lot on the sample size, if we have that the true model is a very complex polynomial then we will in general not conclude that that is the the best model for small sample sizes since our estimate of it will overfit.\n",
        "\n",
        "\n",
        "\n",
        "### Questions/Answers\n",
        "* [Q1]: Can we calculate this one analytically? I mean we have the issue that we work with the MLE estimation in there as well. But that term is probably fixed actually since we want the error for this function. But I am unsure what we do with the $Z_{t-i}$ terms, do we take the expectation over these as well?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WN_wvhNuLlt",
        "colab_type": "text"
      },
      "source": [
        "-------------------------------------------------------------------------\n",
        "Many different model selection criterias have been developed and in this part we will investigate a few of them.\n",
        "\n",
        "Information criterias generally apply to models that are based on likelihood functions and applicable to parametric model settings.\n",
        "* AIC: **In the typical settings where the loss is logarithmic** AIC has the form of\n",
        "$$AIC_m = -2\\hat{l_{n,m}} + 2d_m$$\n",
        "where $\\hat{l_{n,m}}$ is the log-likelihood at the MLE and $d_m$ is the dimension of model $M_m$. This formula has been extended to time series as well and there is also an extension (and a predecessor) called Takeuchi's information criteria which allows for the model to be miss-specified but it is too complex to be used in practice. [Q1] There is also a finite-sample corrected AIC which works when we work with few samples and the model dimension is much larger then the amount of samples available.\n",
        "* BIC: The formula for BIC is given by\n",
        "$$BIC_m = -2\\hat{l_{n,m}} + d_m\\log(n)$$\n",
        "One can show that choosing the model with the smallest BIC is equivalent to choosing the model with the largest marginal likelihood in the asymptotic limit. [Q2]\n",
        "* Hannan and Quinn criterion (HQ): This criterion was proposed as a criterion that acheives strong consisteny in autoregressive order selection, thus if the data was truly generated by an autoregressive model of order $k$ , then we will find it in the asymptotic limit. The method selects a model by minimizing the criterion\n",
        "$$HQ_m = -2\\hat{l_{n,m}} + 2cd_m\\log \\log n$$\n",
        "($c > 1$). **More interestingly it can be proved that no penalty smaller than $2d_m\\log \\log n$ can be strongly consistent**, thus HQ choose the smallest penalty which still yields strong consistency.\n",
        "* Bridge Criterion: This is a criteria that tries and bridge the advantages of both AIC and BIC and thus selects the model that minimizes\n",
        "$$-2\\hat{l_{n,m}} + c_n(1+2^{-1} + ... + d_m^{-1})$$\n",
        "over all models that have a dimension smaller than that chosen from AIC. We can also see that the penalty is of the size $c_n\\log d_m$. This criteria har some nice interpretations as well.\n",
        "There are also other ways of selecting a model such as the model with the highest posterior probability.\n",
        "$$p(M_m|z) = \\frac{p(z|M_m)p(M_m)}{\\sum {p(z'|M)p(M)}}$$\n",
        "We can also use Bayes Factors which are given as\n",
        "$$B_{m,m'} = \\frac{p(M_m|z)/p(M_m)}{p(M_m'|z)/p(M_m')} = \\frac{p(z|M)}{p(z|M')}$$\n",
        "Thus in this case it is easier to compare models without specifying some prior distribution on the space of models.\n",
        "\n",
        "From the Bayes Factor it is also easy to motivate the use of the marginal likelihood or evidence since that is what we would need to calculate if we want to compare several models with the Bayes Factor. **We also see from the formula above that if all prior probabilities are equal then the model with the highest evidence is also the model with the highest posterior probability**. It is important to note though that improper or vague priors can have a large effect on the value of the marginal likelihood.\n",
        "\n",
        "An Information theoretic way to select model is by using **Minimum Message Length** which favours the model that generates the shortest message containing a statement of the model and the data encoded by the model. It selects the model that minimizes\n",
        "$$-\\log p(\\theta) - \\log p(x|\\theta) + \\frac{1}{2}\\log |I(\\theta)| + \\frac{d}{2}(1+\\log \\kappa_d)$$.\n",
        "Since we work with a prior here, MML is in a fully Bayesian setting.\n",
        "\n",
        "Similarly to MML there is MDL (Minimum Description Length). One formulation of this principle is to select the model that minimizes the quantity\n",
        "$$-\\log p_{\\theta_1}(z_1) - \\sum_{t=2}^n \\log p_{\\theta_t}(z_t|z_1,...z_t-1)$$\n",
        "where $p_{\\theta_1}$ is an arbitrarly chosen prior distribution. [Q3]\n",
        "\n",
        "Another criteria is DIC which is a Bayesian model selection criteria. First we define the deviance under model $m$ as $D_m(\\theta) = -2\\log p_m(y|\\theta) + C$ where $C$ is a constant and does not depend on the given model. **We define the effective number of parameters as** (never understood why it has this form) $p_D = \\mathbb{E}_{\\theta |z}(D_m(\\theta)) -D_m(\\mathbb{E}_{\\theta|z}(\\theta))$. Then DIC selects the model $M_m$ that minimizes\n",
        "$$DIC_m = D_m(\\mathbb{E}_{\\theta|z}(\\theta)) + 2p_D$$\n",
        "\n",
        "### Questions/Answers\n",
        "* [Q1]: Where does it come one that we assume that the model is not miss-specified in AIC?\n",
        "* [Q2]: I do not follow the derivation of this in the article, the reason there is no dependency on the prior is maybe due to the asymptotic limit?\n",
        "* [Q3]: Not clear to me exactly how this works."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s11cC8yN6FLX",
        "colab_type": "text"
      },
      "source": [
        "---------------------------------------------------------\n",
        "**The next methods are methods that do not require parametric assumptions**\n",
        "\n",
        "Leave-one-out cross-validation has been shown to be asymptotically equal to AIC or TIC under some regularity conditions. They argue here that after we have gone through all the splits of cross-validation we then retrain the best model on all data, but this does not easily hold for Neural Networks does it now? **There exists fast versions of LOO such as generalized cross-validation** (antar typ det Johans artikel handlade om).\n",
        "\n",
        "**The next section deals with methods proposed for specific applications**\n",
        "* Predictive Least Squares aims to minimize the accumulated prediction errors (in a time series setting), defined as\n",
        "$$PLS_m = \\sum_{t=t_0+1}^n(y_t - x^T_{m,t}\\beta_{m,t-1})^2$$\n",
        "where $x$ is a vector of covariates and $\\beta_{m,t-1}$ is the least squares estimate of the model $m$ based on the data before time $t$. Thus it seems similar to LOO just that we do it in a sequential fashion, and they mention that maybe it ha more similarity with LOO than compared to AIC or BIC. [Q1] The time index $t_0$ is the first index such that $\\beta$ is uniquely defined. But it has been proven that PLS and BIC are asymptotically close.\n",
        "* Generalized Information Criteria: These represent a wide class of criterias that are linear in the model dimension. It selects the regression model $M_m$ that minimizes the expression\n",
        "$$GIC_{\\lambda_n,m} = \\hat{e}_m + \\frac{\\lambda_n \\hat{sigma}_n^2d_m}{n}$$\n",
        "$\\lambda_n$ is a deterministic sequence of $n$ that controls the trade-off between model fitting and model complexity. **Maybe one can take the fitting of Variational Inference inte this framework? But I guess it is similar for NN model with a varying cost constraint.**\n",
        "Under some general constraints one can obtain AIC and BIC and Mallows Cp from this criteria.\n",
        "\n",
        "\n",
        "### Questions/ Answers:\n",
        "* [Q1]: I understand that the time-series nature seems a bit clear from the definition but what can we get if we do this for regular data just that we average it over all possible ways to partition the data? Maybe something similar to how marginal likelihood was equal to cross-validation.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLVMOcKB-qrp",
        "colab_type": "text"
      },
      "source": [
        "------------------------------------------------------------------\n",
        "Some theoretical questions that one can consider in model selection is\n",
        "* Concistency in selection: More in line with finding the model that can explain the patterns or to give insight\n",
        "* Asymptotic efficiency: More in line with prediction\n",
        "* Minimax-rate optimality: More in line with prediction\n",
        "\n",
        "It is also possible to compare model selection criterias based on if they are asymptotically close to BIC or AIC. If the penalty is fixed or goes to infinity are things that will determine which one of these that a criteria is asymptotically close to for example. Since these two criterias are so important we will spend some extra time investigating the theoretical properties of them.\n",
        "\n",
        "It has been shown that AIC is minimax-rate optimal for a range of selection tasks such as subset selection and order selection problems in linear regression, also works well with nonparametric regression based on splines etc. Minimax-rate optimal says that it is the best function in the sense that it performs best in the worst case scenario allowed.\n",
        "\n",
        "**AIC is also asymptotically efficient in a nonparametric framework** (but not in a parametric framework?), i.e, the predictive performance of the selected model is asymptotically equivalent with the best model offered by the candidates.\n",
        "\n",
        "**BIC on the other hand is known to be consistent in selecting the smallest true data-generating process** in a parametric framework. This concistency also implies that BIC is asymptotically efficient in a parametric framework [Q1].\n",
        "\n",
        "We now investigate some more theoretical properties when the sample size is larger than the model dimension.\n",
        "\n",
        "* What we have is that AIC is known to be inconsistent in a parametric framework where there are at least two correct candidate models [Q2]. Thus AIC is not asymptotically efficient in such a framework.\n",
        "* BIC on the other hand does not enjoy minimax-rate optimality and asymptotic efficiency in a parametric framework.\n",
        "\n",
        "The heuristics for why these two criteria are different can be argued as follows:\n",
        "* AIC was based on finding the model that is closest in a Kullback-Leibler sense, since minimizing $KL(p_*||q)$ is equivalent to minimizing $\\mathbb{E}_*(-\\log q)$, AIC is expected to find a distrbution that is good in minimizing the prediction loss (given that we work with the loglikelihood as loss function?). If there is at least one oversized model then AIC will not be consistent due to the fact that the oversized model will reduce the $-\\hat{l}_{n,m}$ by a random quantity which follows a $\\chi^2$ distribution, but the added penalty factor will only be a constant term which will not be able to suppress the random gain by high probability. Thus a larger model can appear just as good as the true model. (Nice to think of it as a random gain I think when we fit the larger model maybe).\n",
        "* For BIC the penalty of $d_m\\log n$ is so much larger than the $2d_m$ for AIC that it cannot enjoy the predictive optimality in a non-parametric framework (I guess there is some connection between that non-parametric models are usually better with more data but then there is a strong penalization etc.)\n",
        "\n",
        "So to summarize:\n",
        "* **For asymptotic efficiency, AIC is only suitable in non-parametric settings where BIC is only suitable in parametric settings.** An issue with this though is that it is not easy to know if the true data generating process lies in a parametric or non-parametric framework.\n",
        "\n",
        "One questions people can have is that if there is a method that is both minimax-rate optimal and consistent, the answer to this questions is no.\n",
        "\n",
        "Another question is that related to if it is possible to have asymptotic efficiency and consistency. **In a parametric framework, concistency is typically equivalent to asymptotic efficiency**. This fusion of these two properties motivated the development of the Bridge Criteria which achieves consistency in a parametric framework and asymptotic efficiency in both non-parametric and parametric frameworks, the idea is to impose a BIC-like penalty for smaller models but when there is evidence that the model is infinitely dimension the penalty is alleivated. \n",
        "\n",
        "**There is also some interest into evalutation if a framework is parametric or non-parametric, one can develop quantities which indicate asymptotically if a framework is parametric or non-parametric.**\n",
        "\n",
        "In practice, it seems AIC is more used compared to BIC since maybe many people have the feeling that most models are wrong. But they argue that BIC can work well when one needs to make inference about certain variables.\n",
        "\n",
        "\n",
        "\n",
        "### Questions/Answers:\n",
        "* [Q1]: I thought it was not possible to have both of these properties at the same time? It doesn't seem to be that way\n",
        "* [Q2]: I do not see how there can be two correct models?\n",
        "* [Q3]: **I Table 1 I do not understand how they calculate the efficiency of the methods.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWIyKMBYKi9a",
        "colab_type": "text"
      },
      "source": [
        "--------------------------------------------------------------------------------------\n",
        "Now they focus on high-dimensional variable selection methods.\n",
        "\n",
        "In many of the proofs for the preceding information in the article, they often assume that the dimnesion $d_m$ is often $o(\\sqrt{n})$. Now we consider the situtation where the dimension can be comparable with $n$, or even larger than $n$.\n",
        "\n",
        "The models that we work with will be given as $y = \\sum \\beta_ix_i + \\epsilon$, and $z = (y,x_1,x_2,...,x_n)$ will be a point.\n",
        "\n",
        "One way to do this to use penalized regression as\n",
        "$$\\hat{\\beta} = \\arg \\min_{\\beta} \\bigg\\{||Y_n - X_n\\beta||_2^2 + \\sum_{j=1}^{d_n}p(|\\beta_j|;\\lambda, \\gamma)\\bigg\\}$$\n",
        "They argue here that it is crucial that the penalty is not differentiable at $\\beta=0$ in order to obtain sparse solutions.[Q1] There is some discussion here if all these different penalizations methods give consistent solutions etc.\n",
        "\n",
        "### Questions/Answers\n",
        "* [Q1]: Why cannot this happen for a differentiable penalty?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fb7R6CIVPSqT",
        "colab_type": "text"
      },
      "source": [
        "----------------------------------------------------------------------\n",
        "Now we will focus on Cross-Validation as a general model selection procedure. So the goal is not to select a given model but instead to select which method that we should use to select a model.\n",
        "\n",
        "So the idea is to split the data into a training and validation set and then running all the different model selection procedures on the training set and then evaluation which one gave the best model on the validation set. It has been shown that doing this for linear models with AIC and BIC will consistently give the right model selection procedure. So in a parametric setting one will get with probability 1 that BIC will be selected as the best procedure while in the non-parametric setting AIC will be selected. [Q1]\n",
        "\n",
        "More data does not lead to a more clear selection of model procedure in this case. This is since similar model selection criterias can give similar performance when more data is added.\n",
        "\n",
        "With cross-validation there are some misconceptions at times. To select a model (and not to select model selction procedure as above), LOO for tuning parameter selection will give the best predictive model for nonparametric regression [Q2]. For selection consistency, 10 fold CV is usually more unstable then 5-fold cross-validation. [Q3]. **k-fold CV is often unstable in the sense that dividing the data into different ways will yield very different results.** One way to alleviate this issue is to repeat the process and take the average validation loss for selection procedure.\n",
        "**For asymptotic efficiency, the ratio between the validation set and training set should either converge to zero or diverge to infinity depending on if we are working in parametric or non-parametric setting**.\n",
        "\n",
        "In order to select the modelling procedure it is often important for the validation set to be quite large (they write half) in order to get a good selection accuracy, LOO is the least trustworthy method in this case.\n",
        "\n",
        "**They do a case study for neural networks and find that the smaller the validation set is the better the network performs on unseen data. So in this case the ratio should go to 0, they say that neural networks are almost non-parametric so this is not surprising.**\n",
        "\n",
        "Then they raise the question of if all models are wrong (or reality is incredibly complicated so a true model is likely not to be considered by us) why do we care about consistency results? They then argue that in many cases, such selection consistency can identify a **stable** model which can then be treated as the true model. (I guess a stable model should have some relation to the true model in some sense that the same factors are relevant and such maybe?). **In a non-parametric regression setting it has been shown that these is no model selection method that can guarantee consistency**. Thus people are usually more interested in asymptotic efficiency (which is a weaker requirement) in nonparametric settings.\n",
        "\n",
        "Some final notes is that they say that model selection is an exploratory process, thus we cannot draw any confirmatory conclusions unless we repeat the experiment (and maybe here the stability of consistency is valuable, that the same model would appear again in some way).\n",
        "* If the object of model selection is inference, then aim for consistency\n",
        "* If the goal is prediction then aim for asymptotic efficiency\n",
        "* If one's goal is prediction, then minimax gives more defense in the worst case scenario, so AIC would be safer than BIC.\n",
        "\n",
        "### Questions/Answers:\n",
        "* [Q1]: But this is in terms of asymptotic efficiency then? So just the best model for prediction then I guess?\n",
        "* [Q2]: What do they mean with tuning parameter here in this case? Is it allowed to control the dimension of the model? Or are we not allowed to use this to compare models of varying sizes?\n",
        "* [Q3]: What do they mean with selection consistency here? That if we repeat and randomize new splits then we would get a different suggested model?"
      ]
    }
  ]
}