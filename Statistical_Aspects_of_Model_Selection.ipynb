{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Statistical Aspects of Model Selection.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antonFJohansson/Article_collection/blob/master/Statistical_Aspects_of_Model_Selection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wD7OAWWZ8s5q",
        "colab_type": "text"
      },
      "source": [
        "--------------------------------------------------------\n",
        "**What did I learn:**\n",
        "\n",
        "They want to find the model $f(y_n;\\theta)$ that minimizes the Kullback-Leibler distance in the sense of (where $g$ is the data generating distribution)\n",
        "$$\\mathbb{E}\\bigg[ KL_n(g(\\cdot),f(\\cdot;\\hat{\\theta}))\\bigg] = \\int g(x_n)\\log \\frac{g(x_n)}{f(x_n;\\hat{\\theta)}}dx_n$$\n",
        "So we want the model that minimizes the Kullback-Leibler distance on average.\n",
        "By assuming certain things such as \n",
        "* Uniqueness of MLE estimate (unique parameters $\\theta*$ such that $\\mathbb{E}[g_n(\\theta*)] = 0$)\n",
        "\n",
        "they can perform a Taylor expansions and use an approximation theorem to obtain \n",
        "$$\\mathbb{E}\\bigg[\\int g(x_n)\\log f(x_n;\\hat{\\theta})dx_n\\bigg] = \\mathbb{E}[l(\\hat{\\theta})] - tr(I_n(\\theta*)J_n(\\theta*)^{-1}) + o(1)$$\n",
        "These terms we can then later try and estimate in order to be able to select a given model. \n",
        "\n",
        "If one assumes that the data generating distribution is in the family that we consider then $I_n(\\theta*) = J_n(\\theta*)$ and the criteria reduces to AIC.\n",
        "\n",
        "While it is possible to estimate the criteria for a few different models one usually has to make some assumptions on the data generating distribution such as that all observations have equal mean etc. It can also be tiresome to work out the specific form of the penalty for all different model families since one has to redo it if one considers a new model family. Therefore they introduce the general form of TIC which is given as\n",
        "$$TIC = -2l(\\hat{\\theta}) +2tr(\\widehat{I}\\widehat{J}^{-1})$$\n",
        "with\n",
        "$$\\widehat{I} = \\sum_{i} \\frac{\\partial l(\\theta)}{\\partial\\theta}\\bigg|_{\\theta=\\hat{\\theta}}\\frac{\\partial l(\\theta)}{\\partial\\theta}^T\\bigg|_{\\theta=\\hat{\\theta}} $$\n",
        "$$\\widehat{J} = -\\sum_i \\frac{\\partial^2}{\\partial \\theta \\partial \\theta'}l(\\theta)\\bigg|_{\\theta=\\hat{\\theta}}$$\n",
        "This estimate can tend to overestimate certain quantities (which I can look into more at some point).\n",
        "\n",
        "They also develop a modified criteria for the case when we have a penalized likelihood so they can find the best penalization parameter $\\lambda$ as well.\n",
        "\n",
        "**What I am wondering about**\n",
        "\n",
        "* Do we really need to assume that there is a unique set of parameters $\\theta*$? It still seems to me that we can do the Taylor expansion and such, the only thing is the other assumptions that might require it?\n",
        "\n",
        "* Assumption A4 plays a big role in how we actually end up estimating things, but where does this even come from? Why would these be similar for large $n$?\n",
        "\n",
        "* Is it possible to use this if we are not in any local minima though but just on the way to converge there? That could be a major restriction. We do get an extra term otherwise which goes away now since we assume that we are in some sort of minima of the loss function. But this one goes away in equation (2.3), maybe it can be good to not assume this and see if it is possible to estimate it in some other way?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYZ1M1frmvV9",
        "colab_type": "text"
      },
      "source": [
        "-----------------------------------------------------------------------------\n",
        "We are interested in selecting a model based on the KUllback-Leibler distance. Modelling is a way of approximating reality, we usually prefer simpler models since these can be easier to understand and also from a viewpoint of overfitting etc. We assume here that we only have access to a limited amount of data that comes from some noisy process. By introducing this randomness into our model we get models that are more robust to fluctations (we can fit our line and just see that the new fluctuated value agrees with the noise that we assumed in our generation process) of the system and also that it becomes more flexible. And another advantage is that we may leave the error in our approximation as part of this random fluctation as long as the approximation error of our model agrees with the error that we assumed. This results in a simplification of the modelling procedure. (I think this is a good way to see it, maybe we can also see here that we can avoid introducing $O(n)$ terms and the like as we do in PDEs? Because we just put all of that into our random errors.)\n",
        "\n",
        "They write that a desirable property of a model selection procedure would be to then reject models that are far from reality and instead pick a model where the approximation error and the error due to random fluctations are well balanced (what does this last part mean here? Do we not want the approximation error to be 0 in some sense? Or maybe they mean that we have reached a point where our model is so good that we do not know if the error is due to the random fluctations or just our approximation error? Further than that we might not be able to improve our model? Some intrinsic limit on how good our model can be.)\n",
        "\n",
        "A practical procedure is to start with a simple model, and then to increase the complexity until a tradeoff between approximation error and random fluctations (This is what they write but it is not completely clear to me, but I guess the procedure is just to increase the complexity of the model until we cannot distinguish between the random fluctations and the approximation error).\n",
        "\n",
        "We do this by introducing information criterions. We do this for the IID setting first since time series and state space models can be quite complicated in this regard."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwgawoF8p8IF",
        "colab_type": "text"
      },
      "source": [
        "------------------------------------------------------------------------------------------\n",
        "The notation that we use is that $Y_n = (y_1,y_2,...,y_n)$ is independent distribution which are not necessarily identically distributed, $g(Y_n)$ is the corresponding joint density and we define a model family as $F = \\{f(Y_n;\\theta ): \\theta \\in \\Theta\\}$.\n",
        "\n",
        "One way to estimate how good our model is is to measure the distance between our model $f(\\cdot ; \\hat{\\theta})$ and the true data generating distribution $g(\\cdot)$. We do this based on our samples $Y_n$, and for now we assume that $\\hat{\\theta} = \\hat{\\theta}(Y_n)$ is the MLE (I was a little bit confused since usually data generating distributions are in GANS or something similar I guess but we always assume that we model the data generating distribution when we do linear regression and such as well, just that to get a prediction we take the conditional mean or something similar).\n",
        "\n",
        "Thus we get the distance\n",
        "$$KL_n(g,f) = \\int g(x_n)\\log \\frac{g(x_n)}{f(x_n;\\hat{\\theta})}dx_n$$\n",
        "(But here we integrate over $x_n$ which is related to the amount of datapoints that we obtained, maybe usually we work in the IID setting and then we can simplify this to the usual setting but now we have to consider the joint of the observed data? So which model fits the data best for these specific random variables sort of? This measure is additive for independent samples, which follows easily by just splitting the joints by independence.)\n",
        "\n",
        "This quantity is random and depends on the specific data that we observed since we use that in our MLE estimate. Thus is we assume that $|\\int g(x_n) \\log g(x_n)dx_n|< \\infty$, then we can rewrite the expectation over the Kullback-Leibler distance as\n",
        "$$\\mathbb{E}\\bigg[KL_n(g,f)\\bigg] = \\int g(x_n) \\log g(x_n)dx_n - \\mathbb{E}\\bigg[\\int g(x_n)\\log f(x_n;\\hat{\\theta})dx_n\\bigg]$$\n",
        "(The only reason we require that the absolute value of the integral is less than infinity is so this difference is well defined?) Since the first term is the same for all model we could use the second term only to compare models, but the issue is that that term depends on $g(\\cdot)$ which is unknown. But the idea would be to choose the model that has the smallest expected Kullback-Leibler distance to the true model (doing it in expectation I think is good since then one sort of removes the influence of this specific choice of observed data. I also think it is an interesting question that it sort of depends on how we find the parameters $\\hat{\\theta}$). What we will show though is that we can approximate this term under some assumptions and that we have many observations. These assumptions are\n",
        "* A1: The parameter space $\\Theta$ is euclidean or an open subspace of it. We also require that the gradient $g_n(\\theta)$ and Hessian $H_n(\\theta)$ (w.r.t $\\theta$) of the loglikelihood $\\log f(\\cdot;\\theta)$ are well defined with probability 1 (non-defined on a set of measure 0?) and also that they are continuous w.r.t $\\theta$.\n",
        "* A2: $\\mathbb{E}[|g_n(\\theta)|] < \\infty$ and $\\mathbb{E}[|H_n(\\theta)|] < \\infty$ where the absolute value $|\\cdot|$ is taken componentwise.\n",
        "* A3: There exists a unique $\\theta* \\in \\Theta$ such that $\\mathbb{E}(g_n(\\theta*)) = 0$ (so we require that there is some form of unique minimum of the loss surface? But how can we apply this then?). For any $\\epsilon > 0$ we have that\n",
        "$$\\sup_{||\\theta - \\theta*||} l(\\theta) - l(\\theta*) \\rightarrow -\\infty~~a.s$$\n",
        "(They do not specify what goes to infinity but I assume it is the number of samples, and what this means then is that the likelihood becomes infinitely peaked at the unique maximizer $\\theta*$.)\n",
        "* A4: For any $\\epsilon>0$ there exists a $\\delta>0$ such that\n",
        "$$\\sup_{||\\theta - \\theta*||<\\delta} |\\mathbb{E}\\bigg((\\hat{\\theta} - \\theta*)J_n(\\theta)(\\hat{\\theta} - \\theta*)\\bigg) - tr(I_n(\\theta*)J_n(\\theta*)^{-1})| < \\epsilon$$\n",
        "for large $n$. Here we have that\n",
        "$$I_n(\\theta*) = \\mathbb{E}\\big[g_n(\\theta)g_n(\\theta)^T\\big]~~~~~~J_n(\\theta*) = -\\mathbb{E}\\big[H_n(\\theta)\\big]$$\n",
        "I am a bit unsure of why we need this last assumption or what is actually says.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQ9zdbYmy5Ki",
        "colab_type": "text"
      },
      "source": [
        "------------------------------------------------------------------------------\n",
        "Assumption 3 above assures us that $\\hat{\\theta} - \\theta*$ converges to 0 almost surely as $n$ tends to infinity. This is since we assume $\\hat{\\theta}$ to be the MLE solution which will then converge to $\\theta*$. That is, $\\hat{\\theta}$ is a consistent estimator of $\\theta*$, consistent here means that the estimator should converge to the desired target in probability as the number of data points go to infinity, since a.s implies convergence in probability we get this.\n",
        "\n",
        "Assumption A3 with assumption A2 implies that $KL_n(g(\\cdot),f(\\cdot;\\theta))$ is minimized at $\\theta*$. **I can see why this would follow from A3, but I do not see why it is necessary to have A2 as well, maybe some requirement on convergence and measurability or something similar?**\n",
        "\n",
        "We can further understand this situation by rewriting the KL-distance as follows\n",
        "$$KL_n(g(\\cdot), f(\\cdot, \\hat{\\theta})) = KL_n(g(\\cdot), f(\\cdot, \\theta*)) + \\int g(x_n)\\log \\frac{f(x_n;\\theta*)}{f(x_n;\\hat{\\theta)}}dx_n$$\n",
        "\n",
        "Thus the first term is a discrepancy due to our model family and how well it can approximate the true data distribution, the second term is the discrepancy that we obtain by using the parameters $\\hat{\\theta}$ instead of $\\theta*$ (I see the second term sort of as a log likelihood ratio and a loglikelihood ratio is often used to see how two distributions compare with each other).\n",
        "\n",
        "(They say that all assumptions we assumed aboe are commonly assumed regularity conditions? So maybe just accept them for now.) We use a Taylor expansion of $\\log f(x_n;\\theta*)$ around $\\hat{\\theta}$. We get\n",
        "$$\\log f(x_n;\\hat{\\theta}) = \\log f(x_n;\\theta*) + (\\hat{\\theta} - \\theta*)\\frac{\\partial \\log f(x_n;\\theta)}{\\partial \\theta}\\bigg|_{\\theta = \\hat{\\theta}} + ...$$\n",
        "One important detail here is that the derivative is not of the loglikelihood w.r.t the observed data $Y_n$, but instead the test sample $x_n$. This means that we have properties such as\n",
        "$$\\int g(x_n)\\frac{\\partial}{\\partial \\theta}\\log f(x_n;\\theta)\\bigg|_{\\theta = \\theta*}dx_n = 0$$\n",
        "This holds due to assumption A3. This means that we can be justified in writing it as\n",
        "$$\\int g(x_n)\\log f(x_n;\\hat{\\theta})dx_n = \\int g(x_n)\\log f(x_n;\\theta*)dx_n + \\frac{1}{2}(\\hat{\\theta} - \\theta*)\\int g(x_n)\\frac{\\partial^2}{\\partial \\theta \\partial \\theta^T}f(x_n;\\theta)\\bigg|_{\\theta = \\xi}dx_n(\\hat{\\theta} - \\theta*)^T$$\n",
        "where $\\xi$ is a value between $\\theta*$ and $\\hat{\\theta}$. Thus using Assumption 4 we obtain\n",
        "$$\\mathbb{E}\\bigg[\\int g(x_n)\\log f(x_n;\\hat{\\theta})dx_n\\bigg] = \\int g(x_n)\\log f(x_n;\\theta*)dx_n - \\frac{1}{2}tr(I_n(\\theta*)J_n(\\theta*)^{-1}) + o(1)$$\n",
        "(They do not speak of the order of convergence of Assumtpion 4 but I guess it is linear at least? Or $o(1)$ just says that there is some function that tends to 0 as $n\\rightarrow \\infty$, so we do not need any $o(n)$ or something similar, and that there is a function that tends to 0 there is given since we know that we have convergence between the two terms. The key idea is that we can approximate away the second integral at least.)\n",
        "\n",
        "Furthermore by expanding the loglikelihood term $l(\\theta*)$ around $\\hat{\\theta}$ we obtain.\n",
        "$$l(\\theta*) = l(\\hat{\\theta}) + \\frac{1}{2}(\\theta* - \\hat{\\theta})H_n(\\xi)(\\theta* - \\hat{\\theta})^T$$\n",
        "(Where we use that $l(\\hat{\\theta}) = 0$ since I guess we assume that the MLE is at a local minimum?). This gives us that\n",
        "$$\\mathbb{E}\\bigg[\\int g(x_n) \\log f(x_n:\\theta*)dx_n\\bigg] = \\mathbb{E}[l(\\hat{\\theta})] - \\frac{1}{2}tr(I_n(\\theta*)J_n(\\theta*)^{-1}) + o(1)$$\n",
        "So what we do is that we insert this expression into $\\mathbb{E}[l(\\theta*)]$ above, all of this gives us that\n",
        "$$\\mathbb{E}\\bigg[\\int g(x_n)\\log f(x_n;\\hat{\\theta})dx_n\\bigg] = \\mathbb{E}[l(\\hat{\\theta})] - tr(I_n(\\theta*)J_n(\\theta*)^{-1}) + o(1)$$\n",
        "Thus we see that we now have some estimate of the term in the beginning and a reasonable model selection choice would then be to select the model based on\n",
        "$$l(\\hat{\\theta}) - \\bar{t_n(\\theta*)}$$\n",
        "where $\\bar{t_n(\\theta*)}$ is some estimate of $tr(I_n(\\theta*)J_n(\\theta*)^{-1})$ (and here if the estimate that we use is unbiased then we would estimate this term without any bias.)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kH__DswN0pH",
        "colab_type": "text"
      },
      "source": [
        "--------------------------------------------------------------------------------\n",
        "There are different ways of estimating the correction term, if the true data generating distribution is among our model family, then $\\theta* = \\theta_0$, $I_n(\\theta_0) = J_n(\\theta_0)$. The last point follows since then we have that\n",
        "$$I_n(\\theta_0) = \\int g(x_n)\\frac{\\partial}{\\partial \\theta_i}\\log f(x_n;\\theta_0)\\frac{\\partial}{\\partial \\theta_i}\\log f(x_n;\\theta_0)dx_n$$\n",
        "And since $g(x_n) = f(x_n;\\theta_0)$ we can insert that and then just differentiate everything and obtain\n",
        "$$I_n(\\theta_0) = \\int f(x_n;\\theta_0)\\frac{1}{f(x_n;\\theta_0)^2}\\frac{\\partial}{\\partial \\theta_i}f(x_n;\\theta)\\frac{\\partial}{\\partial \\theta_j}f(x_n;\\theta)dx_n$$\n",
        "And similarly we can expand $J_n(\\theta_0)$ and obtain that the two terms are equal. Thus if we have this case then the term $tr(I_n(\\theta_0)J_n(\\theta_0)^{-1}) = p$ where $p$ is the amount of parameters in the model. Thus we get the Akaike Information Criterion from here which we can use when the data generating distribution is in the considered model family (or very close to it for a good approximation) (but for neural networks we can approximate a wide variety of functions but Akaike still does not work, I mean we do not have a unique MLE but does the proof really break down there?)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGMqsWy6tpCu",
        "colab_type": "text"
      },
      "source": [
        "--------------------------------------------------------------\n",
        "Now let us try this for a model family where we do not assume the data generating distribution to be, such as the normal location and scale family given by $F(y_i;\\theta) = 1/\\sqrt{2\\pi\\sigma^2}\\exp(-(y_i-\\mu)^2/(2\\sigma^2))$, where $\\theta = (\\mu,\\sigma)$. Thus we seek the parameters $\\theta*$ that give us $\\mathbb{E}[g_n(\\theta*)] = 0$. Differentiating the log likelihood we obtain\n",
        "$$g_n(\\theta) = \\bigg(\\frac{\\sum_{i=1}^n(y_i-\\mu)}{\\sigma^2}, -\\frac{n}{\\sigma} + \\frac{\\sum_{i=1}^n(y_i - \\mu)^2}{\\sigma^3}\\bigg)$$\n",
        "This can be solved for the parameters $\\theta*$ to obtain that\n",
        "$$\\mu* = \\frac{1}{n}\\sum \\mathbb{E}[y_i]~~~~~~\\sigma^2* = \\frac{1}{n}\\sum\\mathbb{E}[(y_i - \\mu*)^2]$$\n",
        "where the expectation is taken w.r.t the data generating distribution. Thus we get that $I_n(\\theta*) = \\mathbb{E}[g_n(\\theta*)g_n(\\theta*)^T]$. Another assumption they make regarding the data generating distribution here is that all the $y_i$ are independent, they have the same first and second moments. This enables to to get the matrix I_n(\\theta*) as (since we know that $\\mu* = \\mathbb{E}[y_i]$ for example),\n",
        "$$I_n(\\theta*) = \\begin{bmatrix}\n",
        "    \\frac{1}{\\sigma^2*} & ...  \\\\\n",
        "    ... & ...\n",
        "  \\end{bmatrix}$$\n",
        "This gives us an expression for $\\bar{t_n(\\theta*)}$ where we estimate $\\sigma^2*$ by MLE and another parameter in another way. And we can do something similar for other models as well, but one downside with this is that it is tedious to do this for again and again for different model families and also for the different assumptions that one might have one the data generating distribution. They also show a situation where it is easier to upper bound the penalty term $t_n(\\theta*)$ then to try and assume things about the data generating distribution such as equal second moments etc. This upper bound has the property that for models that do fit the data well then the bound is close to being tight, thus we do not change the model selection process in any major way.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAr6smwe4eth",
        "colab_type": "text"
      },
      "source": [
        "-------------------------------------------------------------\n",
        "In order to avoid having to redo all of this for a new model and also start assuming things about our data generating process, we define the general TIC to be that we approximate the matrices $I_n(\\theta*)$ and $J_n(\\theta*)$ and then use those approximations. We estimate those matrices as ($l$ is the loglikelihood and we assume all datapoints to be independent)\n",
        "$$\\widehat{I} = \\sum_{i} \\frac{\\partial l(\\theta)}{\\partial\\theta}\\bigg|_{\\theta=\\hat{\\theta}}\\frac{\\partial l(\\theta)}{\\partial\\theta}^T\\bigg|_{\\theta=\\hat{\\theta}} $$\n",
        "$$\\widehat{J} = -\\sum_i \\frac{\\partial^2}{\\partial \\theta \\partial \\theta'}l(\\theta)\\bigg|_{\\theta=\\hat{\\theta}}$$\n",
        "So we just use the MLE estimate.\n",
        "This gives the general definition of TIC as \n",
        "$$TIC = -2l(\\hat{\\theta}) +2tr(\\widehat{I}\\widehat{J}^{-1})$$\n",
        "This estimate can tend to overestimate the penalty term at times. They can also extend this to penalized maximum likelihood estimators and get a very similar criteria."
      ]
    }
  ]
}