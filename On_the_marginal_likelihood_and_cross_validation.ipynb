{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "On the marginal likelihood and cross-validation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antonFJohansson/Article_collection/blob/master/On_the_marginal_likelihood_and_cross_validation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wBElEP7tOPZ",
        "colab_type": "text"
      },
      "source": [
        "We are dealing with the task of choosing a model. Non-Bayesian ways are usually done by performing cross-validation. To get the true estimate for leave-p-out CV we should fit the model $n\\choose{p}$ times. This is usually approximated instead with leave-k-out CV.\n",
        "\n",
        "In Bayesian statistics it is common to choose model $M$ based on the marginal likelihood/evidence which gives is given as\n",
        "$$p(y_{1:n}|M):=p_M(y_{1:n}) = \\int f_{\\theta}(y_{1:n})d\\pi(\\theta)$$\n",
        "where $\\pi$ is our prior, $f$ the given likelihood parametrized by the parameter $\\theta$. This can also be used to calculate the posterior probability of a given model as\n",
        "$$p(M|y_{1:n})  = \\frac{p(y_{1:n}|M)p(M)}{p(y_{1:n})} \\propto p(y_{1:n}|M)p(M)$$\n",
        "The denominator is the probability of the observed data under all possible models $M$ and is thus constant. Thus we can use this to compare two model by using the Bayes factor which compares posterior to prior odds of observing the data as [Q1]\n",
        "$$\\frac{p(M_1|y_{1:n})}{p(M_2|y_{1:n})}\\frac{p(M_2)}{p(M_1)} = \\frac{p(y_{1:n}|M_1)}{p(y_{1:n}|M_2)}$$\n",
        "The marginal likelihood can be sensitive to the choice of prior and therefore it is also common to select a model based on BIC or DIC. They start some discussion here about $M$-open models where we say that we do not consider the true model to be among the models considered. The marginal likelihood can still be used for model selection in such a situation but is considered as a scroing rule instead then [Q2]. [But this part is something interesting and something I should go back to.]\n",
        "\n",
        "We will work here with a general Bayesian framework where we do not require that the true model is among those considered (it seems similar to obtaining the parameter closest to the true distribution in a KL-sense). We define the parameter of interest as \n",
        "$$\\theta_0 = \\arg \\min_{\\theta} \\int l(\\theta,y)dF_o(y)$$\n",
        "where $F_0$ is the true data generating distribution, $l$ is a loss function linking the parameter $\\theta$ to the observation $y$. In order to have a coherent scoring rule we want the property that\n",
        "* Under exchangeability where indices carry no information, identical models should score the same equally irregardless of which order the data arrived in.\n",
        "Some argue that a coherent update rule of the beliefs about $\\theta_0$ to the posterior after observing $y_{1:n}$ should have the form [Q3]\n",
        "$$\\pi_G (\\theta| y_{1:n}) \\propto \\exp (-wl(\\theta, y_{1:n}))\\pi_G(\\theta)$$\n",
        "where $l$ is a loss function and $w>0$ a loss-scale parameter. We can obtain the usual Bayesian update rule by choosing $w=1$ and $l = -\\log p(y_{1:n}|\\theta)$.\n",
        "\n",
        "An update function $\\psi (l(\\theta,y),\\pi_G(\\theta)) = \\pi_G(\\theta|y)$ is coherent if for some inputs $y_{1:2}$ it satisfies [Q4]\n",
        "$$\\psi[l(\\theta,y_2),\\psi (l(\\theta,y_1),\\pi_G(\\theta))] = \\psi(l(\\theta,y_1) + l(\\theta,y_2), \\pi_G(\\theta))$$\n",
        "This means we should perform the same update if we observe first $y_1$ then $y_2$ or if we observed both at once. We would now like to extend this coherence princciple to the Bayesian selection of models, where the goal is to evaluate a fit of the observed data under a model class $M_G = \\{l(\\theta, y) : \\theta \\in \\Theta\\}$ with a prior $\\pi_G(\\theta)$ [Q5]. We define the log posterior predictive score as\n",
        "$$s_G(\\tilde{y}|y_{1:n}) = \\log \\int g[l(\\theta, \\tilde{y})]d\\pi_G(\\theta|y_{1:n})$$\n",
        "where $g: \\bar{\\mathbb{R}}^+ \\rightarrow \\bar{\\mathbb{R}}^+$ is monotonically decreasing scoring function [Q6]. We define cumulative prequential log score as\n",
        "$$S_G(y_{1:n}) = \\sum_{i=1}^ns_G(y_i|y_{1:i-1})$$\n",
        "with the notation that $s_g(y_1|y_{1:0}) = \\log \\int g(l(\\theta,y_1))d\\pi_G(\\theta)$. \n",
        "\n",
        "They now define coherency for a model scoring function as:\n",
        "* A model scoring function $g$ is coherent if it satisfies [Q7]\n",
        "$$\\sum_{i=1}^n s_G(y_i|y_{1:i-1}) = \\log \\int g[l(\\theta, y:{1:n})]d\\pi_G(\\theta)$$ for all $\\Theta$, $\\pi(\\theta)$ and $n>0$\n",
        " such that $S_G(y_{1:n})$ is invariant to the ordering or the partitioning of the observations. Maybe if one reads the article where they define the coherent rule for updating the posterior one can gain an intuition for why this is a coherent rule here.\n",
        " \n",
        " Their proposition then states that the unique scoring rule $g$ is given by $g(l) = \\exp(-wl)$, and here we can see some connection to what they talked about in the introduction. This also gives us that if we work in a Bayesian inference setting with the negative log likelihood as our loss function, we obtain the logarithm of the marginal likelihood as our scoring rule. This follows straightforwardly. [Q8]\n",
        "## Questions/Answers\n",
        "* [Q1]: I know I have thought of this before but it is a bit strange for me to base a model selection criteria on how likely the data is to be generated under the prior, would feel better to be the posterior. But I guess it is some weighing between how well the data can fit at all under the possible parameter values and how likely we believe they are.\n",
        "* [Q2]: Is there some issue with seeing the models simply as my belief? I do not see why we would have to deal with $M$-open models etc if we have this perspective, the only thing might be how we approach it later,e.g, that the KL might not be able to reach 0 and such I guess.\n",
        "* [Q3]: I guess it should be $\\theta_0$ in the formula and not $\\theta$?\n",
        "* [Q4]: For some inputs? Not for all inputs? And it should also hold in the opposite direction with 1:2 right?\n",
        "* [Q5]: But so here their goal is to choose which model parameters that they would like to use? And not to choose between different models, like different networks? Or I guess maybe that can be incorporated into the loss function in some way? But what does it mean to extend this coherence principle here as well? That we would select the same model irregardless of the way we observe the data?\n",
        "* [Q6]: They write that $g$ should transform $l(\\theta,y)$ into a predictive score but is that not already what the loss function does?\n",
        "* [Q7]: I do not see where this would come from or what the intuition behind it would be actually.\n",
        "* [Q8]: So does this then show that if we want a **coherent** rule for choosing our model then the only choice we have is to work with something related to the marginal likelihood?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yEFXTTtSn8f",
        "colab_type": "text"
      },
      "source": [
        "-----------------------------------------------\n",
        "We now want to connect marginal likelihood and cross-validation with each other.\n",
        "\n",
        "The leave-p-out cross validation score is given as\n",
        "$$S_{CV}(y_{1:n};p) = \\frac{1}{n\\choose{p}}\\sum_{t=1}^{n\\choose{p}}\\frac{1}{p}\\sum_{j=1}^p s(\\tilde{y_j}^t|y_{1:n-p}^t)$$\n",
        "**They mention here that LOOCV is asymptotically inconsistent but does this not contradict the other article by Vehtari?** Usually a large p will penalize complexity since for small datasets it is easy for the methods to overfit. From a Baeysian perspective they mention that it can be natural to consider the log posterior predictive function as the scoring function, $s(\\tilde{y}|y) = \\log \\int f_{\\theta}(\\tilde{y})d\\pi(\\theta|y)$, we saw earlier as well that this is the only coherent scoring rule to work with. The main proposition is now that \n",
        "* Bayesian Marginal likelihood is equivalent to sum over leave-p-out CV **using the log posterior predictive as scoring rule**, such that\n",
        "$$\\log p_M (y_{1:n}) = \\sum_{p=1}^n S_{CV}(y_{1:n};p)$$ with the scoring rule given as $s(\\tilde{y_j}|y_{1:n-p}) = \\log p_M (\\tilde{y_j}|y_{1:n-p}).$ Thus we see that by using the marginal likelihood we can take into consideration all possible subsets of data that we can leave out during CV.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfyZJtoXY0-q",
        "colab_type": "text"
      },
      "source": [
        "---------------------------------------------\n",
        "From this they can reason a bit about how sensible the marginal distribution is to the specification of the prior. The last term in the sum is given as\n",
        "$$S_{CV}(y_{1:n};n) = \\sum s(\\bar{y}_j^t|y^t_{1:0})$$\n",
        "where we earlier defined it such that\n",
        "$$s(\\bar{y}_j^t|y^t_{1:0}) = \\log \\int f_{\\theta}(y_j)d\\pi(\\theta)$$\n",
        "Thus we can see that the contribution from these terms is wholly determined by how well we specify the prior. It might be undesirable to have a model selection criteria dependent on how well we specify the prior, one can instead argue that maybe it is better to gauge the fit of the method on a part of the data points. So that instead of considering the full sum, we sum up to a value $P$, such that we consider\n",
        "$$S_{ACV}(y_{1:n};P) = \\sum_{i=1}^P S_{CV}(y_{1:n};i)$$\n",
        "(and it also leads to a cross-valdidation score of how well the prior fits by summing from $P+1\\rightarrow n$). They have some suggestion for how this $P$ should be chosen, such that 90% of the training data is left, or dependent on the parameters etc. By rewriting it like this we lose the property of coherency that they desired, but this can be obtained again by conditioning the marginal likelihood on these observed data points, so we get the new score as\n",
        "$$S_{ACV}(y_{1:n};P) = \\frac{1}{n\\choose{P}}\\sum_{t=1}^{n\\choose{P}} \\log p_M(\\bar{y}^t_{1:P}|y^t_{1:{n-P}})$$\n",
        "This is apparently related to something known as the log geometric intrinsic Bayes factor which was developed to issues that might arise in the marginal likelihood due to improper priors, where the remedy is to use part of the data to make the prior proper, similarly to how we use part of the data now to not make our model choice rely so much on our prior specification. While the last expression above is difficult to compute they say that it might be possible to approximate it through some Monte Carlo and MCMC sampling.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjJt3q3sYlk6",
        "colab_type": "text"
      },
      "source": [
        "-----------------------------------------------------------------------------------------\n",
        "The last part will deal with the proofs in the Appendix\n",
        "\n",
        "**A1** If I understand the proof of the first Appendix then they first create a simple situation in which they can show that there is a unique $g$ and that it has the desired form. And then they simply show that this $g$ also fulfills the equation for all $n$ and $\\theta$ after that. So I guess what they want to show is not that for a fix $n$ that there is a unique $g$ that solves the problem, but instead that if we require this $g$ to work for all $n$,$\\Theta$ etc then it must be this $g$ since no other $g$ works in their simple constructed case in the beginning.\n",
        "\n",
        "**A2** We consider an ($!n \\times n$) matrix $Z$ with elements $(Z)_{ti} = \\log p_M(y_i^{(t)}|y_{1:i-1}^{(t)})$, thus a row in the matrix is given by\n",
        "$$\\log p_M(y_1^{(t)}|y_{1:1-1}^{(t)}),\\log p_M(y_2^{(t)}|y_{1:2-1}^{(t)}),...,\\log p_M(y_n^{(t)}|y_{1:n-1}^{(t)})$$\n",
        "where in this case the index 1 refer to the actual observation $1$ etc. The full matrix is obtained where each row is one of the possible $n!$ permutations of $y_{1:n}$. The sum of each row is identical due to the chain rule of probability and the sum equals $\\log p_M(y_1,y_2,...,y_n)$. Hence we get that\n",
        "$$\\log p_M(y_{1:n}) = \\frac{1}{n!}\\sum_{t=1}^{n!}\\sum_{i=1}^n(Z)_{ti} = \\sum_{i=1}^n\\frac{1}{n!}\\sum_{t=1}^{n!}(Z)_{ti}$$.\n",
        "Due to exchangeability, in each column the value of $(Z)_{ti}$ is invariant to the permutations of the preceding $y_{1:i-1}$ values. This is since the value of $(Z)_{ti} = \\log p_M(y_i^{(t)}|y^{(t)}_{1:i-1}) = p_M(y_i^{(t)},y^{(t)}_{1:i-1}) / p_M(y^{(t)}_{1:i-1}))$ is invariant to permutations due to exhcangeability. Thus there are $n$ choose $i-1$ unique training sets for each $y_i$ and given the training set there are $n-i+1$ unique choices for the $y_i$ [Q9]. Thus for each column we can write \n",
        "$$\\frac{1}{n!}\\sum_{t=1}^{n!}(Z)_{ti} = \\frac{1}{n\\choose{i-1}}\\sum_{t=1}^{n\\choose{i-1}}\\frac{1}{n-i+1}\\sum_{j=1}^{n-i+1}s(y_j^{(t)}|y^{(t)}_{1:i-1}) = S_{CV}(y_{1:n};n-i+1)$$\n",
        "The reason one sum sums over $n!$ terms and the other one sums over less terms is that in the full ($n! \\times n$) matrix there are a lot of rows which will yield the exact same value due to exchangeability and permutations. If we are interested in the fourt column for example, then all of these rows would yield the same value.\n",
        "\\begin{align}\n",
        "&y1,y2,y3,y4,y5,y6\\\\\n",
        "&y2,y1,y3,y4,y5,y6\\\\\n",
        "&y1,y2,y3,y4,y6,y5\\\\\n",
        "&y3,y2,y1,y4,y6,y5\n",
        "\\end{align}\n",
        "This yields that there are $(i-1)!\\times (n-i)!$ extra values for each unique one in the left sum. Thus in the right sum we can see that we can multiply each term in the sum with that quantity since\n",
        "$$\\frac{n!}{(i-1)!(n-i+1)!} \\times(n-i+1) = \\frac{n!}{(i-1)!(n-i)!}$$\n",
        "Thus we see that we get the desired result if we let $p=n-i+1$, namely that\n",
        "$$\\log p_M (y_{1:n}) = \\sum_{p=1}^n S_{CV}(y_{1:n};p)$$\n",
        "## Questions/Answers\n",
        "* [Q9]: Would the result matter if there are multiple $y_i$ with the exact same value? It does not feel like it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-j641xxomGkO",
        "colab_type": "text"
      },
      "source": [
        "----------------------------------------------------------\n",
        "**A3** An alternative proof for the same proposition as in **A2** can be obtained as follows:\n",
        "\n",
        "For the preparatory cross-validation score given as\n",
        "$$S_{PCV}(y_{1:n};P) = \\sum_{p=P+1}^n S_{CV}(y_{1:n};p)$$\n",
        "We have the following equality\n",
        "$$S_{PCV}(y_{1:n};P) = \\frac{1}{n\\choose{P}}\\sum_{t=1}^{n\\choose{P}}\\log p_M (y_{1:n-P}^{(t)})$$\n",
        "First we will prove this fact by using induction. The equality is trivially true for $P=n-1$ since then the two terms give\n",
        "\\begin{align}\n",
        "&\\frac{1}{n\\choose{n-1}}\\sum_{t=1}^{n\\choose{n-1}}\\log p_M (y_{1:n-n+1}^{(t)}) = \\frac{1}{n}\\sum_{t=1}^n\\log p_M (y_{1:1}^{(t)})\\\\\n",
        "&S_{PCV}(y_{1:n};n-1) = S_{CV}(y_{1:n};n) = \\frac{1}{n}\\sum_{j=1}^n\\log p_M(y_j|y_{1:0})\n",
        "\\end{align}\n",
        "[If we interpret the term $p_M(y_j|y_{1:0})$ as $p_M(y_j)$ then equality will follow in a straightforward way, since in the first term we just sum over all the way we can permute one observation which is the same as going through them one-by-one.]\n",
        "\n",
        "Thus we have showed the base-case and now we assume that the equality holds for some $P$, $1 \\leq P \\leq n-1$, thus we have that\n",
        "$$S_{PCV}(y_{1:n};P-1) = S_{PCV}(y_{1:n};P) + S_{CV}(y_{1:n};P)$$\n",
        "This equality follows easily.\n",
        "$$=\\frac{1}{n\\choose{P}}\\sum_{t=1}^{n\\choose{P}}\\log p_M(y_{1:n-P}^{(t)}) + \\frac{1}{n\\choose{P}}\\sum_{t=1}^{n\\choose{P}}\\frac{1}{P}\\sum_{j=1}^P\\log p_M (y_j^{(t)}|y_{1:n-P}^{(t)}) $$\n",
        "And by extracting the common factors we obtain\n",
        "$$\\frac{1}{P}\\frac{1}{n\\choose{P}}\\sum_{t=1}^{n\\choose{P}}\\bigg(P\\log p_M(y_{1:n-P}^{(t)})+\\sum_{j=1}^P\\log p_M (y_j^{(t)}|y_{1:n-P}^{(t)})\\bigg)$$\n",
        "From the properties of conditional probability, we get\n",
        "$$S_{PCV}(y_{1:n};P-1) = \\frac{1}{P}\\frac{1}{n\\choose{P}}\\sum_{t=1}^{n\\choose{P}}\\sum_{j=1}^{P}\\log p_M (y_j^{(t)}, y_{1:n-P}^{(t)})$$\n",
        "This is simply obtained by expnanding the conditional in the expression above. Once again we will use the fact that the marginal likelihood is invariant under permuations of the $y_j$.\n",
        "\n",
        "Now, similar to before, we will consider the amount of repetitions that we sum over in these sums. In the last sum we consider terms such as \n",
        "\\begin{align}\n",
        "&y_{p1},y_{t1},...,y_{t(n-P)}\\\\\n",
        "&y_{p2},y_{t1},...,y_{t(n-P)}\\\\\n",
        "&y_{p3},y_{t1},...,y_{t(n-P)}\\\\\n",
        "&...\\\\\n",
        "&y_{pP},y_{t1},...,y_{t(n-P)}\\\\\n",
        "\\end{align}\n",
        "So we see that we cannot throw the $y$-values around in anyway.\n",
        "Now if we consider sequences of length $n-P+1$, then each such sequence can be partitioned into a sequence of the form $y_j,y_{1:n-P}$ in $n-P+1$ ways. We get this amount of ways since the all we can do is just decide which one to put in the first spot,a dnt his can be done in $n-P+1$ ways. This can also be seen as \n",
        "$$P n\\choose{n-P} = (n-P+1)n\\choose{n-P+1}$$\n",
        "from which we can see that if we want to change the sum then we need this extra factor to account for some of the terms. Thus we see that we get\n",
        "$$S_{PCV}(y_{1:n};P-1)= \\frac{n-P+1}{Pn\\choose{P}}\\sum_{t=1}^{n\\choose{P-1}}\\log p_M(y_{1:n-P+1}^{(t)}) = \\frac{1}{n\\choose{P-1}}\\sum_{t=1}^{n\\choose{P-1}}\\log p_M(y_{1:n-P+1}^{(t)})$$\n",
        "And thus by induction we get the desired proposition. The proposition 2 follows now easily by taking $P=0$ in what we proved."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7K1mv1N8rs47",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkcIAVKytMXi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}