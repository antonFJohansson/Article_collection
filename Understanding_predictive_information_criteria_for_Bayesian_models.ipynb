{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Understanding predictive information criteria for Bayesian models.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antonFJohansson/Article_collection/blob/master/Understanding_predictive_information_criteria_for_Bayesian_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sK0lqz-Fv0o",
        "colab_type": "text"
      },
      "source": [
        "-----------------------------------------------\n",
        "This article will review some information criteries and frame them in a Bayesian way. The considered information criterias are\n",
        "* AIC\n",
        "* DIC\n",
        "* WAIC\n",
        "\n",
        "And also some investigation into cross-validation. What we want is to estimate the expected out-of-sample error based on a bias-corrected version of the within-sample-error. [Q1]\n",
        "\n",
        "All of these information criterias can be viewed as an approximation to some form of cross-validation [They do not show this in the article though]. There are some controversies with all these methods (mainly DIC) but that is because they attempt at obtaining an unbiased estimate of the true prediction error, given only the data used to fit the algorithm, and they have to be quick to compute so that people will actually use them for their methods. In this article they will focus on the metric given by predictive accuracy and this will determine which of the models to select.\n",
        "Throughout this article we will work with a likelihood for our data as \n",
        "$$p(y|\\theta,x) = \\prod_{i=1}^np(y_i|\\theta,x_i)$$\n",
        "We will most of the time suppress the dependency on $x$ in our notations.\n",
        "\n",
        "There are two kind of measures for predictive accuracy that we can be interested in, one for point estimates and one for probabilitistic predictions. Measures of predictive accuracies for point estimates are called scoring functions (e.g mean squared error), for probabilistic predictions they are called scoring rules (e.g logarithmic and zero-one scores).[Q2] \n",
        "\n",
        "A common summary of the predictive fit is the loglikelihood/log-predictive-density. The theory behind this rule is that in the limit of high-data we have the property that the model with the smallest KL to the true data generating distribution will also have the highest expected loglikelihood. [Q3] Thus we can use this as a criteria for how well the model fits the data [Q4].\n",
        "\n",
        "## Questions/Answers\n",
        "* [Q1]: I guess we say that it is bias corrected because the estimated error will have been shifted a bit from its true position due to using within-sample-error. And not bias in the sense that the model cannot fit the data?\n",
        "* [Q2]: Why is zero-one score a probabilistic scoring rule? Is it because we try and predict a few different classes and we only get 0 loss when predicting the right class? In that case I guess also cross-entropy is a scoring rule.\n",
        "* [Q3]: They mention here that this also gives that the model will have the highest posterior probability\n",
        "* [Q4]: I do not understand how we would use this in a Bayesian setting though, we do not have a single parameter value $\\theta$ that we can plug in right?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AzBFdKnNZ0x",
        "colab_type": "text"
      },
      "source": [
        "--------------------------------------------------\n",
        "Under standard conditions the posterior density will approach a normal distribution in the limit of high amounts of data. In this limit the posterior is dominated by the likelihood since it contributes $n$ terms while the prior only contributes one term, the likelihood will therefore also be of Normal density shape. Denoting the asymptotic Normal distribution as $\\theta|y \\sim \\mathcal{N}(\\theta, V_0/n)$, then we get the expression for the loglikelihood as\n",
        "$$\\log p(y|\\theta) = c(y) -\\frac{1}{2}(k \\log(2\\pi)+\\log|V_0/n| + (\\theta-\\theta_0)^T(V_0/n)^{-1}(\\theta-\\theta_0))$$\n",
        "This follows since we have\n",
        "$$\\frac{p(y|\\theta)}{p(y)} \\approx \\frac{p(y|\\theta)p(\\theta)}{p(y)} = p(\\theta|y)$$\n",
        "in the large data limit. From this we can see that since $\\theta|y \\sim \\mathcal{N}$, the loglikelihood will have a density given by a $\\chi_d^2$ density, where $d$ is the dimension of $\\theta$. For singular models where a set of different parameters can map to the same model, this aysmptotic result above will not hold. One needs to analyze these models in a different way.\n",
        "\n",
        "The ideal measure of a models predictive accuracy would be its performance on new unseen data, i.e, out-of-sample prediction performance for data generated from the same distribution $f$ that generated the training data. The out-of-sample predictive fit for a new datapoint is given as\n",
        "$$\\log p_post(\\tilde{y}) = \\log \\mathbb{E}_{post}(p(\\tilde{y}|\\theta)) = \\log \\int p(\\tilde{y}|\\theta)p_{post}(\\theta)d\\theta$$\n",
        "But since we do not know the new datapoint $\\tilde{y}$ we must define a new quantity know as expected-(out-of-sample)-log-predictive-density (elpd) (also known as mean log predictive density) as\n",
        "$$elpd = \\mathbb{E}_f(\\log p_{post}(\\tilde{y})) =\\int \\log p_{post}(\\tilde{y}) f(\\tilde{y})d\\tilde{y}$$\n",
        "Now they also say that in order to keep comparability with the current data set we can define \"expected log pointwise predictive density for a new dataset\" (elpdd) which is given as [Q5]\n",
        "$$elpdd = \\sum_{i=1}^n\\mathbb{E}_f(\\log p_{post}(\\tilde{y_i}))$$\n",
        "And sometimes we can do all of the above given a point estimate for our parameter $\\theta$, $\\hat{\\theta}$.\n",
        "\n",
        "\n",
        "Now this might connect with some of the things I mentioned above but they write that in general the parameter $\\theta$ is not known and thus we would like to work with the posterior distribution $p(\\theta|y)$ instead. Then we could summarize the model fit with the log pointwise predictive accuracy as\n",
        "$$lppd = \\log \\prod_{i=1}^n p_{post}(y_i) = \\sum_{i}\\log \\int p(y_i|\\theta)p_{post}(\\theta)d\\theta$$\n",
        "Given posterior samples, we can easily evaluate this with Monte Carlo sampling as\n",
        "$$lppd \\approx \\sum_{i=1}^n\\bigg(\\frac{1}{S}\\sum_{j=1}^Sp(y_i|\\theta_j)\\bigg)$$\n",
        "We will see that this term is an underestimate of the true lppd since we only use the observed data, i.e, the error on our training data will make our model appear better than it actually is. Thus we are interested in some form of bias correction for this term. There are still some questions regarding how to implement this, and it can depend on how our experiment is performed, is the number of new observations a random variable or fixed, are we working in a hierarchical model or not etc. To take these effects into account in a cross-validation scheme one can use something called h-block cross-validation. [Variations similar to this but for Information Criterias have not been proposed].\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Questions/Answers:\n",
        "* [Q5]: I do not see how we can get different things in each term in this sum, it should all be the same in my opinion?\n",
        "* [A5]: I think there could be a difference to consider $n$ datapoints at one time, given that we do not assume independence between them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INFnWsBxZjXS",
        "colab_type": "text"
      },
      "source": [
        "----------------------------------------------------------\n",
        "Usually Information Criterias are based on a measure of the Deviance which is the log predictive density given a point prediction for the parameters multiplied with -2, i.e, $-2\\log p(y|\\hat{\\theta})$ If two models are equally complex then one can simply compare their log predictive densities straight away to get an idea of which model is the best (is this true though? I guess they do not deal with singular models or model with multiple minimas maybe), but when the models are of different sizes it is important to somehow penalize the complexity since a higher capacity model always is able to fit the training data better. Since we do not have the true data generating distribution we have to approximate the log predictive density in some way. We can do that as\n",
        "* Use the observed training data.\n",
        "* Adjust for bias, such as in AIC where we subtract a term based on the bias. These methods are **correct only in expectations** and thus we cannot say anything in a single case when we compare two different methods [Q6].\n",
        "* Corss-validation. CV is also tied to the data at hand and can thus only at best be correct in expectations. Is this since we cannot know which data we actually obtained and the data we obtained can prefer other methods?\n",
        "\n",
        "**AIC** For AIC we work with point estimates of the parameters, we thus try and estimate the quantity of the expected log predictive density $\\log \\mathbb{E}_fp(\\tilde{y}|\\hat{\\theta}(y))$, where both $\\tilde{y}$ and $y$ are random. This is not possible to estimate since we do not know $f$ and thus we instead work with the observed data and try and correct for the bias. AIC relies on asymptotic normality of the likelihood and the by subtracting the number of parameters $k$, we subtract an estimate for how much $k$ parameters would increase the predictive accuracy by chance alone. Thus we obtain the formula as\n",
        "$$\\hat{elpd}_{AIC} = \\log p(y|\\hat{\\theta}_{MLE}) - k$$\n",
        "AIC is the above multiplied with -2. This formula does not work easily beyond linear models with flat priors, this is since how to measure the number of parameters is not completely clear (there are extensions which deal with effective number of parameters but these are apparently difficult to work with).\n",
        "\n",
        "\n",
        "**DIC** DIC works in a similar way to AIC but instead of the MLE estimate, we take the posterior mean estimate of $\\theta$. And instead of a penalty based on the number of parameters, DIC involves a penalty based on the data, thus the new term is given as\n",
        "$$\\hat{elpd}_{DIC} = \\log p(y|\\hat{\\theta}_{postmean}) - p_{DIC}$$\n",
        "where $p_{DIC}$ is a measure of the number of effective amount of parameters and is defined as\n",
        "$$p_{DIC} = 2\\bigg(\\log p(y|\\theta_{postmean}) -\\mathbb{E}_{post}(\\log p(y|\\theta))\\bigg)$$\n",
        "Thus we can easily sample the last expectation to get an estimate of the effective number of parameters. They write [The posterior mean of $\\theta$ will produce the maximum log predictive density when it happens to be same as the mode, but I do not really see this. Should it not be maximum if $\\theta$ is given by the MLE estimate, per definition. Unless they mean the whole expression elpd.] There is a slightly adjusted formula as well given by\n",
        "$$p_{DICalt} = 2var_{post}(\\log(p(y|\\theta)))$$.\n",
        "The advantage with the alternative measure is that it is always positive, the original $p_{DIC}$ can be negative. The first one is more numerically stable though. For linear models with a uniform prior distribution, both of these measures reduce to the number of parameters $k$ [Q7]. The reason this measure was made popular was due to easy evalutation through MCMC samples. The actual DIC is measured with deviance as before to obtain, \n",
        "$$DIC = -2\\log p(y|\\hat{\\theta}_{Bayes}) + 2p_{DIC}$$\n",
        "\n",
        "## Questions/Answers\n",
        "* [Q6]: But I guess there are possibilities to sample new data or something similar and see which model is preferred in expectations with these methods then?\n",
        "* [Q7]: They say that these measures reduce to $k$ for linear models but this is not completely clear to me."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyF-cVzqOvuR",
        "colab_type": "text"
      },
      "source": [
        "-------------------------------------------------------------------------------------\n",
        "There is also the criteria known as WAIC (Widely Applicable Information Criteria). This is a fully Bayesian approach to estimate the out-of-sample expectation. It has a similar structure to the earlier ones, a datafit term (log pointwise posterior predictive) and a penalty term for the amount of parameters. There are two ways to calculate these penalty terms, the first is given as\n",
        "$$p_{WAIC1} = 2\\sum_{i=1}^n\\bigg(\\log (\\mathbb{E}_{post} p(y_i|\\theta)) - \\mathbb{E}_{post}(\\log p(y_i|\\theta))\\bigg)$$\n",
        "Which is easily calculated by simulations as before, The other correction term is given as\n",
        "$$p_{WAIC2} = \\sum_{i=1}^nvar_{post}(\\log p(y_i|\\theta))$$\n",
        "Which can be estimated with the sample variance. We then can use either of these terms as the bias correction and obtain\n",
        "$$\\hat{elppd}_{WAIC} = lppd - p_{WAIC}$$\n",
        "Once again if we are working with a linear model one can show that this is approximately the same as a penalty on the number of parameters. There are also some connections between this one and cross-validation. One advantage of WAIC is that we do not rely on any point estimate when working with this formula, we use the full posterior distribution. WAIC also works with singular models.\n",
        "\n",
        "A difference between WAIC and DIC compared to AIC is that these have penalty terms that depend on the observed data. This can make sense in certain situations such as for example, when working with a model $y_1,...y_n \\sim \\mathcal{N}(\\theta,1)$ and a prior $p(\\theta) = U(0,\\infty)$. If we observe all $y$ close to 0, then there is almost nothing to estimate since the parameter $\\theta$ must be close to 0, thus the effective number of parameter will be less than 1 (I imagine the extreme case when we observe only 0 values, then $\\theta$ must be 0 in some sense and we cannot estimate it). While on the other hand if we observe high values for all $y$, then there is still uncertainty left in the parameter $\\theta$ and the effective number of parameters is 1 [Q8].\n",
        "\n",
        "There is also the BIC measure but they say that the goal of BIC is to simply estimate the marginal distribution of the data and does not rely on estimating the out-of-sample accuracy. They do not consider it further.\n",
        "\n",
        "\n",
        "\n",
        "## Questions/Answers\n",
        "* [Q8]: They mentioned online that effective number of parameters can be easily seen if we are working in regularisation, such as L1 where even if we have many parameters, some of them has to be forced to 0 and we actually do not have that many parameters. Can we see this similarly here but instead with the uncertainty sort of, how much the uncertainty is reduced about one parameter given that we know another?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5UyCNaPV7Rw",
        "colab_type": "text"
      },
      "source": [
        "----------------------------------------------------------------------\n",
        "There is also the possibility of working with Bayesian Cross-Validation where we split the data into $y_{train}$ and $y_{holdout}$, we train our model on $y_{train}$, obtain a posterior distribution and use that distribution to predict the values on the hold out set as $\\log p_{train}(y_{holdout}) = \\log \\int p(y_{holdout}|\\theta)p(\\theta|y_{train})$. This can the be approximated with Monte Carlo sampling.\n",
        "We focus here on the LOO-CV in which we have the term given as \n",
        "$$lppd = \\sum_{i=1}^n \\log p_{post(-i)}(y_i)$$\n",
        "which we can calculate with Monte Carlo sampling as before. Each prediction is conditioned on $n-1$ datapoints which will thus underestimate the predictive fit (since we use less data than what we have available). For large $n$ this is not an issue but for small $n$ it can cause some issues. There are ways to correct for this bias but it is rarely used. There are some shortcuts to avoid having fit $n$ models here but we will not deal with them in this article.\n",
        "\n",
        "There are also some relations between cross-validation and these information criterias, such as\n",
        "* AIC has been shown to be asymptotically equivalent to LOO-CV computed using the maximum likelihood estimate.\n",
        "* DIC is a variation of the **regularized information criteria** which has been shown to be asymptotically equivalent to LOO-CV using plug-in predictive densities\n",
        "* Bayesian CV also works with singular models and is asymptotically equivalent with WAIC, for small $n$ there can be a significant difference.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wk-6cuafaP77",
        "colab_type": "text"
      },
      "source": [
        "--------------------------------------------------------------------------------------\n",
        "In this part we will evaulate these criterias in some simple situations. First we consider Normal data with a uniform prior distribution as $y_1,...,y_n \\sim \\mathcal{N}(\\theta,1)$ with a noninformative prior distribution given as $p(\\theta) \\propto 1$.\n",
        "* AIC: The formula is given as\n",
        "$$\\hat{elpd}_{AIC} = \\log p(y|\\hat{\\theta}_{MLE}) - k$$\n",
        "The MLE estimate is given by $\\theta = \\bar{y}$ and the log predictive density is given by\n",
        "$$\\log p(y|\\hat{\\theta}_{MLE}) = -\\frac{n}{2}\\log(2\\pi) - \\frac{1}{2}\\sum_{i=1}^n(y_i-\\bar{y})^2 = -\\frac{n}{2}\\log(2\\pi) - \\frac{1}{2}(n-1)s_y^2$$\n",
        "Thus the AIC value is given by\n",
        "$$\\hat{elpd}_{AIC} = -\\frac{n}{2}\\log(2\\pi) - \\frac{1}{2}(n-1)s_y^2 - 1$$\n",
        "[**It is interesting that the prior term actually has no effect at all here.**]\n",
        "* DIC: The formula is given as\n",
        "$$\\hat{elpd}_{DIC} = \\log p(y|\\hat{\\theta}_{postmean}) - p_{DIC}$$\n",
        "where \n",
        "$$p_{DIC} = 2\\bigg(\\log p(y|\\theta_{postmean}) -\\mathbb{E}_{post}(\\log p(y|\\theta))\\bigg)$$\n",
        "Thus we need the posterior mean and the posterior distribution to estimate the expectations. We can derive the posterior density to be\n",
        "$$p(\\theta|y) = \\mathcal{N}(\\theta;\\bar{y},\\sqrt{1/n}^2)$$\n",
        "Calculating the term given as (the first term is identical with the first AIC term)\n",
        "$$\\mathbb{E}_{post}(\\log p(y|\\theta)) = -\\frac{1}{2}\\log(2\\pi)-\\frac{1}{2}[(n-1)s_y^2+1]$$. Thus the total term for DIC is\n",
        "$$\\hat{elpd}_{DIC} = -\\log p(y|\\hat{\\theta}_{postmean}) + \\mathbb{E}_{post}(\\log p(y|\\theta)) =  -\\frac{n}{2}\\log(2\\pi) - \\frac{1}{2}(n-1)s_y^2 - 1$$\n",
        "So AIC = BIC in this case (but mostly because we didn't add any prior information I presume).\n",
        "* WAIC: The formula for WAIC is given as\n",
        "$$\\hat{elpd}_{WAIC} = \\sum_{i=1}^n \\log p_{post}(y_i) - p_{WAIC}$$\n",
        "So first we need the posterior predictive distribution. This distribution can be obtained to be $p_{post}(y) = \\mathcal{N}(y|\\bar{y},1+\\frac{1}{n})$, thus summing these terms for all data points we obtain\n",
        "$$\\sum_{i=1}^n -\\frac{n}{2}\\log(2\\pi)-\\frac{n}{2}\\log(1+\\frac{1}{n}) - \\frac{1}{2}\\frac{n(n-1)}{n+1}s_y^2$$\n",
        "Next we will determine the two forms of effective number of parameters which are given as,\n",
        "$$p_{WAIC1} = 2\\sum_{i=1}^n\\bigg(\\log (\\mathbb{E}_{post} p(y_i|\\theta)) - \\mathbb{E}_{post}(\\log p(y_i|\\theta))\\bigg)$$\n",
        "and\n",
        "$$p_{WAIC2} = \\sum_{i=1}^nvar_{post}(\\log p(y_i|\\theta))$$\n",
        "For the first term we obtain\n",
        "$$2\\sum_{i=1}^n\\bigg(\\log p(y_i|y) + \\frac{1}{2}(\\log(2\\pi)+(y_i-\\bar{y})^2+\\frac{1}{n})\\bigg) = 2\\sum_{i=1}^n\\log p(y_i|y) + 2\\frac{n}{2}\\log(2\\pi)+(n-1)s_y^2+1$$\n",
        "Combining this gives\n",
        "$$p_{WAIC1} = \\frac{n-1}{n+1}s_y^2+1-n\\log(1+\\frac{1}{n})$$\n",
        "And similarly we can get $p_{WAIC2}$ as\n",
        "$$p_{WAIC2} = \\frac{n-1}{n}s_y^2+\\frac{1}{2n}$$\n",
        "So we can see that there are differences between WAIC and AIC and DIC. The effective number of parameters for WAIC is less than 1, in the limit of large $n$ we can replace $s_y^2$ by it's expected value of $1$ (we have assumed the data generating distribution to be the normal model and $s_y^2$ is an unbiased estimator of the variance) which gives $p_{WAIC} \\rightarrow 1$.\n",
        "* Cross-validation: For LOO-CV we want the posterior predictive $p_{post-i}(y_i) = \\mathcal{N}(y_i;\\bar{y}_{-i},1+1/(n+1))$, thus we get the sum of these terms as\n",
        "$$\\sum_{i=1}^n \\log p_{post-i}(y_i) = \\frac{n}{2}\\log (2\\pi)-\\frac{n}{2}\\log(1+\\frac{1}{n-1}) - \\frac{1}{2}\\frac{n-1}{n}\\sum_{i=1}^n(y_{-i}-\\bar{y}_{-i})^2$$\n",
        "It is also possible to add a bias corrected term to this expression [Q9]\n",
        "\n",
        "## Questions/Answers\n",
        "* [Q9]: This bias correction was for the fact that we estimate these terms using only $n-1$ data points and $n$ data points right? \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pek8z1v6SWz6",
        "colab_type": "text"
      },
      "source": [
        "-------------------------------------------------------\n",
        "We now want to compare these terms with the expectation of these quantities over new data. Thus since we know the generative distribution we are interested in evaluating\n",
        "$$elppd = \\sum_{i=1}^n\\mathbb{E}(\\log p_{post}(\\tilde{y}_i)) = -\\frac{n}{2}\\log(2\\pi)-\\frac{n}{2}\\log(1+\\frac{1}{n})-\\frac{1}{2}\\frac{n}{n+1}\\sum_{i=1}^n\\mathbb{E}\\big((\\tilde{y}_i-\\bar{y})^2\\big)$$\n",
        "Thus we are interested in the expected value of the quantities that we try and evaluate. The last term we can evaluate as (expectation is taken w.r.t both the new datapoints $\\tilde{y}$, but also those already observed $\\bar{y}$,so we calculate the expected value when we repeat the entire experiment and calculate on new points.)\n",
        "$$\\mathbb{E}\\bigg(\\sum_{i=1}^n(\\tilde{y}_i-\\bar{y})^2 \\bigg) = (n-1)\\mathbb{E}(s_y^2) +n\\mathbb{E}((\\bar{\\tilde{y}}-\\bar{y})^2)$$\n",
        "We rewrite the sum such that the $\\tilde{y}$ are subtracted towards the mean of those points, and the second term is just the difference of the means. Now we have assumed these points are obtained iid and both the new and old points have the same distribution and thus we get\n",
        "$$\\bar{y} \\sim \\mathcal{N}(\\theta, 1/n)~~~~\\textrm{and}~~~~\\mathbb{E}((\\bar{\\tilde{y}}-\\bar{y})^2) = \\mathbb{E}(\\bar{\\tilde{y}}^2)-\\mathbb{E}(2\\bar{\\tilde{y}}\\bar{y}) + \\mathbb{E}(\\bar{y}^2) = 2var(\\bar{y}) = \\frac{2}{n}$$\n",
        "Thus the entire expectation above gives $n+1$. And thus we obtain\n",
        "$$elppd = -\\frac{n}{2}\\log(2\\pi)-\\frac{n}{2}\\log\\big(1 + \\frac{1}{n}\\big) - \\frac{n}{2}$$\n",
        "We will also need the expected value of the log pointwise prediction density for existing data, i.e\n",
        "$$\\mathbb{E}(lppd) = \\mathbb{E}(\\sum_{i=1}^n\\log p_{post}(y_i)) = -\\frac{n}{2}\\log(2\\pi)-\\frac{n}{2}\\log(1+\\frac{1}{n}) - \\frac{1}{2}\\frac{n(n-1)}{n+1}\\mathbb{E}(s_y^2)$$\n",
        "where the last term is equal to 1. So it is important that elppd is not the same as $\\mathbb{E}(lppd)$, one works with future data and the other considers the data that we have observed. I think one difference lies in that the point that we predict over $y_i$ in lppd, is also contained in the corresponding mean $\\bar{y}$, and this is not the case for elppd.\n",
        "\n",
        "The correct number of effective number of parameters is the difference between these two quantities, i.e, [Q10]\n",
        "$$\\mathbb{E}(lppd) - elppd = \\frac{n}{n+1}$$\n",
        "and this quantitity will always be less than 1. \n",
        "\n",
        "For AIC and DIC we are interested in the expected out-of-sample log density given a point estimate, this we also calculate\n",
        "$$\\mathbb{E}(\\log p(\\tilde{y}|\\hat{\\theta}(y))) = \\frac{n}{2}\\log(2\\pi)-\\frac{n}{2}-\\frac{1}{2}$$\n",
        "By taking the expectation of the predicted quantities from $elpd_{AIC}$ and $elpd_{DIC}$ it is possible to show that these are unbiased and thus give the quantiity above. When comparing with the true elppd though there is a difference\n",
        "\n",
        "## Questions/Answers\n",
        "* [Q10]: Why is this a good measure of the effective number of parameters? I can understand that this would be a measure of how the bias correction sort of.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPuUoYT-hdyL",
        "colab_type": "text"
      },
      "source": [
        "----------------------------------------------------------------------\n",
        "The final part deals with the discussion. They say that their first step in a modelling problem is setting up a simple model, such as a normal model, even if one knows data is discreet etc. After that we can expand on our model and see how it can be improved, these model selection criterias can then be used to see how well the expanded model compares.\n",
        "\n",
        "It is also not completely clear how large of a value of these information criterias that correspond to a large gain in an improved model, sometimes the gain can be small and sometimes it can be large.\n",
        "\n",
        "These information criterias and corss-validation try to correct for the fact that we use the data twice, once to fit our models and then to select the model. When these models are used for model selection there can be an issue, if we are choosing between many different models using this as a criteria then it is possible that we obtain s subpar model due to overfitting. They therefore view these procedures as a tool for understanding models and not for selecting between models.\n",
        "\n",
        "The final comments regard some issues with these criterias:\n",
        "* AIC: This does not work in situations where we have some strong prior information\n",
        "* DIC: Does not give good results/nonsensical results when the posterior distribution is not well-summarized by its mean (maybe not good for Neural networks then?)\n",
        "* WAIC: Relies on data partition which can make it tricky to apply in structured data settings.\n",
        "* CV: Can be computationally expensive and can also not be well defined in dependent data settings (How can it not be well defined?)\n",
        "They thus mention that sometimes Bayesian do not always use predictive error comparisons but it has its place due to it being a useful tool to compare models.\n",
        "Their preferred choice is Cross-validation with WAIC as a fast and computationally efficient alternative.\n",
        "\n",
        "They would like to see some research into bridging WAIC and CV so that one has the efficiency of the first one and the robustness of the second one.\n",
        "\n",
        "My main question regarding this article is some intution for why the penalty term takes the shape as it does, we can see that it is unbiased in the limit and such but they look a bit foreign to me in some cases (except AIC)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGsyGRJoFobr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}